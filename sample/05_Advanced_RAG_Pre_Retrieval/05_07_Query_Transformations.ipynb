{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-retrievers-bm25 llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"query-transforms\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "When handling user queries in a RAG system, agent, or any other pipeline, there are various ways to transform and decompose the queries before executing them.\n",
    "\n",
    "One way is query rewriting. This involves rewriting the original query in multiple ways while which then sent sent for retrieval and generation. \n",
    "\n",
    "LlamaIndex implements various query transformations, [check the source code for details](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-legacy/llama_index/legacy/indices/query/query_transform/base.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "QUERY_GEN_PROMPT = \"\"\"Users aren't always the best at articulating what they're looking for. Your task is to understand the \n",
    "essense of the user query and generate {num_queries} alternate queries to expand the users query so it's more robust. This way the user will\n",
    "recieve the most relevant information. \n",
    "\n",
    "Examples are delimited by triple backticks (```) below\n",
    "\n",
    "````\n",
    "User Query: How can I find the positive in situations that seem negative?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I cultivate optimism and positive thinking in my daily life?\n",
    "2. Is it possible to find meaning and purpose in challenging or difficult times?\n",
    "3. What are some effective strategies for reframing negative thoughts into positive ones?\n",
    "````\n",
    "\n",
    "````\n",
    "User Query: How do I deal with setbacks, failures, delays, defeat, or other disasters?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I build resilience and learn to cope with adversity effectively?\n",
    "2. What are some practical tips for overcoming challenges and obstacles that I face?\n",
    "3. How can I develop a growth mindset and view setbacks as opportunities for learning?\n",
    "4. What are healthy ways to process and learn from failures and mistakes?\n",
    "````\n",
    "````\n",
    "User Query: How can I overcome defeat and suffering by changing my mindset?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. What is the power of positive thinking and affirmations, and how can they benefit me?\n",
    "2. Can mindfulness and meditation practices improve my mental well-being and outlook?\n",
    "3. How can I develop self-compassion and acceptance, especially during difficult times?\n",
    "```\n",
    "\n",
    "Generate {num_queries} alternate queries, one on each line, for the following user query:\\n\n",
    "--------------------\n",
    "User Query: {query}\\n\n",
    "--------------------\n",
    "\n",
    "Alternate Queries:\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE = PromptTemplate(QUERY_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_STRING = \"How can I create my own luck?\"\n",
    "\n",
    "def generate_queries(query= QUERY_STRING, llm=Settings.llm, num_queries  = 4):\n",
    "    response = llm.predict(\n",
    "        QUERY_GEN_PROMPT_TEMPLATE, \n",
    "        num_queries=num_queries, \n",
    "        query=query\n",
    "        )\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "\n",
    "generate_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SubQuestionQueryEngine`\n",
    "\n",
    "The `SubQuestionQueryEngine` works by breaking down a complex query into simpler sub-questions (with each potentially targeting a specific data source).\n",
    "\n",
    "#### Here's how it works:\n",
    "\n",
    " - The `SubQuestionQueryEngine` receives a complex query.\n",
    "\n",
    "- It then decomposes this query into several sub-questions. Each sub-question is designed to extract specific information from a particular data source.\n",
    "\n",
    "- The engine then sends these sub-questions to their respective data sources and gathers the responses.\n",
    "\n",
    "- Finally, it synthesizes all the intermediate responses to form a final comprehensive answer to the original complex query.\n",
    "\n",
    "This process makes the `SubQuestionQueryEngine` particularly useful for handling compare/contrast queries across documents, as well as queries pertaining to a specific document. It's also well-suited for multi-document queries and can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"the senpais\",\n",
    "            description=\"The collective thoughts and writings of all my virtual mentors\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sub_question_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True\n",
    "    )\n",
    "\n",
    "sub_question_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict\n",
    "\n",
    "sub_q_prompts = sub_question_query_engine.get_prompts()\n",
    "\n",
    "display_prompt_dict(sub_q_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_question_query_engine.query(\"How can I build my own luck, what are the types of luck I should pursue, and how can I hack luck and minimize my exposure to downside while maintaining skin in the game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "At a high level, [HyDE](https://arxiv.org/pdf/2212.10496.pdf) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example. \n",
    "\n",
    "- 🧐 **Problem Tackled**: Addresses the struggle of creating fully zero-shot dense retrieval systems without relevance labels.\n",
    "\n",
    "- 📚 **Traditional Methods**: Rely on relevance labels for document retrieval based on semantic similarities.\n",
    "\n",
    "- 🚫 **Zero-Shot Challenge**: Especially tough without a large dataset for training.\n",
    "\n",
    "### What is HyDE?\n",
    "\n",
    "Given a query, `HyDE` instructs a language model to generate a hypothetical document.\n",
    "\n",
    "This document captures relevance patterns but might contain inaccuracies or false details.\n",
    "\n",
    "After generating the hypothetical document, an unsupervised contrastively learned encoder encodes the document into an embedding vector.\n",
    "\n",
    "This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity.\n",
    "\n",
    "### How Does HyDE Work?\n",
    "\n",
    "The process starts by feeding a query to a generative model with the instruction to \"write a document that answers the question\". This generates a hypothetical document that captures the essence of relevance.\n",
    "\n",
    " - Generates an embedding vector for a \"fake\" document\n",
    "\n",
    "- It does not generate any actual text content for the document\n",
    "\n",
    "- The embedding is solely for reserving space in the vectorstore index\n",
    "\n",
    "- There is no full hypothetical document text you can access later\n",
    "\n",
    "This vector is used to search against the corpus embeddings, and the most similar real documents are retrieved. The idea is that a hypothetical answer to a question is more semantically similar to the real answer than the question is. \n",
    "\n",
    "**In practice this means that your search would use GPT to generate a hypothetical answer, then embed that and use it for search**.\n",
    "\n",
    "Key advantages of HyDE:\n",
    "\n",
    "- Zero-shot, no labeled data or fine-tuning needed\n",
    "\n",
    "- Performs comparably to fine-tuned retrievers across tasks/languages\n",
    "\n",
    "- Grounds the query in real data via generated hypothetical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "\n",
    "from llama_index.core.query_engine import TransformQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(\n",
    "    include_original=True,\n",
    "    )\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine = index.as_query_engine(), \n",
    "    query_transform = hyde,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt_dict(hyde_query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = hyde_query_engine.query(QUERY_STRING)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from utils import create_query_pipeline\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "hyde_chain = [input_component,  hyde_query_engine]\n",
    "\n",
    "hyde_query_pipeline = create_query_pipeline(hyde_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_query_pipeline.run(input=\"What should I do if I feel like I am not being true to myself and relying too much on external securities?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
