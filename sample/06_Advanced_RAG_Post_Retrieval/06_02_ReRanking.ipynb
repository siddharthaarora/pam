{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"node-reranking\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of the LlamaIndex Order of Operations\n",
    "\n",
    "In LlamaIndex, the order of operations in the query pipeline typically follows these steps:\n",
    "\n",
    "**üçΩÔ∏è Data Ingestion:** This is where your existing data from various sources and formats (APIs, PDFs, SQL, etc.) is ingested into the system.\n",
    "\n",
    "**üóÇÔ∏è Data Indexing:** The ingested data is structured into intermediate representations that are easy and performant for Large Language Models (LLMs) to consume.\n",
    "\n",
    "**üêï Retrieval:** Information is retrieved from your data sources based on the question or prompt. This is the first step in the Retrieval-Augmented Generation (RAG) process.\n",
    "\n",
    "**üéñÔ∏è Reranking:** The initially retrieved documents or nodes are reordered based on certain criteria to bring the most relevant or useful nodes to the top.\n",
    "\n",
    "**‰∑æ Post-processing:** After retrieval and reranking, transformations or filters are applied to the set of nodes to further refine them before they are used to generate the final response.\n",
    "\n",
    "**üí¨ Response Generation:** The LLM generates a response based on the enriched prompt, which now includes the context from the retrieved and reranked documents.\n",
    "\n",
    "We've already talked about post-processing, now let's discuss re-ranking!\n",
    "\n",
    "# Reranking\n",
    "\n",
    "In LlamaIndex, reranking and post-processing are two different steps in the query pipeline.\n",
    "\n",
    "Reranking is a process that takes the initial set of retrieved nodes (documents or pieces of information) and reorders them based on some criteria. This could be based on a model's prediction of relevance, a time-based factor, or any other custom criteria. \n",
    "\n",
    "The goal of reranking is to bring the most relevant or useful nodes to the top of the list.\n",
    "\n",
    "On the other hand, post-processing is a step that happens after the retrieval and reranking steps. \n",
    "\n",
    "It involves applying transformations or filters to the set of nodes. This could include filtering out nodes below a certain similarity score, applying a time decay factor, or any other custom transformation. The goal of post-processing is to further refine the set of nodes before they are used to synthesize the final response.\n",
    "\n",
    "Both reranking and post-processing involve manipulating the set of retrieved nodes, they serve different purposes and occur at different stages in the query pipeline. \n",
    "\n",
    "### Reranking is about ordering the nodes, while post-processing is about transforming or filtering the nodes.\n",
    "\n",
    "By far, the most popular reranking technique is using Cohere's Rerank model. And that's the only one we'll cover in this section.\n",
    "\n",
    "### **‚ÑπÔ∏è Note:** Rerankers have the same usage pattern as post processors:\n",
    "\n",
    "\n",
    "```python\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.postprocessor import YourPostProcessOfChoice\n",
    "\n",
    "... # prior steps to this point: index is defined\n",
    "\n",
    "your_post_processor = YourPostProcessOfChoice(WhateverArgumentsYouNeedToPass)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    ..., #all your othe query engine arguments\n",
    "    node_postprocessors = your_post_processor\n",
    ")\n",
    "\n",
    "# for just a single query\n",
    "response = query_engine.query(\"your query\")\n",
    "\n",
    "# use the query pipeline and apply to your entire dataset if you'd like\n",
    "```\n",
    "\n",
    "Keep in mind that order does matter when you put a reranker and a postprocessor into a query engine. The order in which you list them determines the order in which they are applied.\n",
    "\n",
    "# [Cohere Rerank](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/postprocessor/llama-index-postprocessor-cohere-rerank/llama_index/postprocessor/cohere_rerank/base.py)\n",
    "\n",
    "The `CohereRerank` postprocessor  reranks a list of nodes based on their relevance to a given query using the Cohere API. It enhances the relevance of the retrieved nodes by applying the Cohere reranking model.\n",
    "\n",
    "Be sure you install it, like so `pip install llama-index-postprocessor-cohere-rerank`\n",
    "\n",
    "#### Arguments you need to know\n",
    "\n",
    "- `top_n`: The number of top nodes to return (default is 2).\n",
    "\n",
    "- `model`: The name of the Cohere model to use. Default is `\"rerank-english-v2.0\"`, however I suggest using the latest model: `\"rerank-english-v3.0\"`.\n",
    "\n",
    "- `api_key`: The Cohere API key. If not provided, it tries to retrieve the key from the `COHERE_API_KEY` environment variable.\n",
    "\n",
    "\n",
    "#### Under the hood\n",
    "\n",
    "The reranker is added as a postprocessor to the query engine. When a query is made, the retrieved nodes will be reranked using the Cohere API based on their relevance to the query, and the top 3 nodes will be returned.\n",
    "\n",
    "1. When the postprocessor is called with a list of nodes and a query bundle, it extracts the text content from each node.\n",
    "\n",
    "2. It uses the Cohere API's reranking functionality to rerank the nodes based on their relevance to the query.\n",
    "\n",
    "3. The Cohere API returns a list of reranked results, each containing the index of the original node and a relevance score.\n",
    "\n",
    "4. The postprocessor creates a new list of `NodeWithScore` objects based on the reranked results, preserving the original node and updating the relevance score.\n",
    "\n",
    "5. The reranked list of nodes is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "postprocessor = CohereRerank(\n",
    "    top_n=5, model=\"rerank-english-v3.0\", api_key=CO_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_engine\n",
    "from utils import create_query_pipeline\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=15,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    post_processor=postprocessor,\n",
    "    return_source=True\n",
    "    )\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "query_chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(query_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How can I ensure that my mind is always the sharpest it can be?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline.run(input=\"How can I ensure that my mind is always the sharpest it can be?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most popular alternatives to Cohere rerank are:\n",
    "\n",
    "- [Colbert Rerank](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/ColbertRerank/). You can install this as `pip install llama-index-postprocessor-colbert-rerank` and import as `from llama_index.postprocessor.colbert_rerank import ColbertRerank`.\n",
    "\n",
    "- [Flag Embedding Reranker](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/FlagEmbeddingReranker/). You can install this as `pip install llama-index-postprocessor-flag-embedding-reranker` and import as `from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker`.\n",
    "\n",
    "\n",
    "Both have the same usage pattern as above, and you can refer to the source code for the specific arguments.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
