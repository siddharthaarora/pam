{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 openai==1.30.1 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-oepnai==0.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OPENAI_API_KEY key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    model=\"gpt-4o\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(senpai_documents[42].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ‚Üíüî∑ Small to Big Retrieval ‚óæÔ∏è ‚Üí ‚¨õÔ∏è\n",
    "\n",
    "The concept of small to big retrieval, also known as recursive retrieval, is a key part of LlamaIndex. And, in order to use this, we need to define how to efficiently retrieve relevant context from an index based on a query. That means defining a recursive retrieval strategy, post processing the nodes once they've been retrieved and synthsizing the responses. \n",
    "\n",
    " 1) üîÑ **Recursive Retrieval**\n",
    "\n",
    "  - **Small Chunks (Child Chunks)**: Initially retrieves smaller, query-specific chunks of data.\n",
    "\n",
    "  - **Big Chunks (Parent Chunks)**: Follows references to larger, contextual chunks related to the smaller chunks. Retains context within each chunk.\n",
    "\n",
    "  2) üõ†Ô∏è **Node Postprocessing:** Apply transformations, filtering, or re-ranking to the retrieved nodes to enhance data quality and relevance.\n",
    "  \n",
    "  3) üìù **Response Synthesizer:** Use the retrieved text chunks along with the user query to generate a response\n",
    "\n",
    "\n",
    "## [ü™ü`SentenceWindowNodeParser`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/sentence_window.py)\n",
    "\n",
    "The `SentenceWindowNodeParser` is unique in that it focuses on individual sentences while also capturing the surrounding context.  This is particularly useful for tasks where understanding the broader context of a sentence is useful.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. **Sentence Splitting:** \n",
    "\n",
    "    *   Similar to `SentenceSplitter`, it first divides the document into individual sentences using a sentence tokenizer (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library).\n",
    "\n",
    "2. **Window Creation:**\n",
    "\n",
    "    *   For each sentence (node), it gathers a \"window\" of surrounding sentences based on the specified `window_size`. \n",
    "\n",
    "    *   This window is stored in the node's metadata under the `window_metadata_key`.\n",
    "\n",
    "3. **Metadata Management:**\n",
    "\n",
    "    *   The original sentence text is also stored in the metadata under `original_text_metadata_key`.\n",
    "\n",
    "    *   Importantly, both the window and original text are excluded from being seen by the embedding model and LLM.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`window_size`**: Controls the number of sentences to include before and after the central sentence in the window.\n",
    "\n",
    "*   **`window_metadata_key`**: The key used to store the window text in the node's metadata.\n",
    "\n",
    "*   **`original_text_metadata_key`**: The key used to store the original sentence text in the metadata.\n",
    "\n",
    "*   **`sentence_splitter`**: The text splitter to use when splitting documents (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library).\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "parser = SentenceWindowNodeParser(window_size=2)\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "### When to Use `SentenceWindowNodeParser`\n",
    "\n",
    "*   **Tasks requiring sentence-level understanding with context:** \n",
    "    *   Question answering, summarization, or sentiment analysis where the surrounding sentences provide valuable context.\n",
    "\n",
    "*   **Fine-grained control over embedding scope:** \n",
    "    *   Creating embeddings that focus on the specific meaning of a sentence within its local context.\n",
    "    \n",
    "*   **Combining with MetadataReplacementNodePostProcessor:**\n",
    "    *   Replacing the original sentence with its surrounding window before sending it to the LLM, allowing the model to consider the broader context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senpai_documents[42].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "example_parsed = SentenceWindowNodeParser(window_size=2).build_window_nodes_from_documents([senpai_documents[42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_parsed[3].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_parsed_2 = SentenceWindowNodeParser(window_size=3).get_nodes_from_documents([senpai_documents[42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_parsed_2[3].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Understanding the `MetadataReplacementPostProcessor` and `SentenceWindowNodeParser`**\n",
    "\n",
    "- üìù **`SentenceWindowNodeParser` Review**\n",
    "\n",
    "  - **Single Sentence Parsing**: Parses documents into nodes, each containing a single sentence.\n",
    "\n",
    "  - **Contextual Window**: Each node includes a \"window\" of sentences surrounding the core sentence for added context.\n",
    "\n",
    "- üîÑ **[`MetadataReplacementPostProcessor`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/postprocessor/metadata_replacement.py)**\n",
    "\n",
    "  - **Context Enhancement**: Replaces the sentence in each node with its surrounding window of sentences during retrieval.\n",
    "\n",
    "  - **Used in Conjunction**: Often paired with the `SentenceWindowNodeParser` to maximize contextual data provided to the LLM (Language Learning Model).\n",
    "\n",
    "### Query and Response Process\n",
    "\n",
    "- üîç **Query Handling**\n",
    "\n",
    "  - **Sentence Retrieval**: Retrieves the most relevant sentences based on the query.\n",
    "\n",
    "  - **Context Injection**: Instead of merely returning these sentences, the post-processor injects the surrounding context from the window.\n",
    "\n",
    "- üìä **Benefits of Enhanced Context**\n",
    "\n",
    "  - **Improved Understanding**: More context helps the LLM understand queries better, leading to more accurate responses.\n",
    "\n",
    "  - **Detailed Responses**: The additional context allows for responses that are both detailed and relevant.\n",
    "\n",
    "- üåü **Ideal for Large Documents**\n",
    "\n",
    "  - **Fine-Grained Retrieval**: Especially useful for large documents or indexes, enabling more precise information extraction.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/0*JKZ9m_c6jyIKqCWu.png\">\n",
    "\n",
    "Image Source: [Ivan Ilin](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "def sentence_window_splitter(window_size, documents):\n",
    "    splitter = SentenceWindowNodeParser(\n",
    "        window_size=window_size,\n",
    "        window_metadata_key=\"window_size\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = sentence_window_splitter(window_size=5, documents=senpai_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[5].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[5].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[5].get_content(metadata_mode=\"llm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è Ingest to Qdrant And Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine\n",
    "from utils import setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"wots-small-to-big-sentence-window\"\n",
    "\n",
    "sentence_window_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest\n",
    "\n",
    "transforms = [Settings.embed_model]\n",
    "\n",
    "split_nodes = ingest(\n",
    "    documents=nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=sentence_window_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "from utils import create_query_engine\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "node_postprocessors = [MetadataReplacementPostProcessor(target_metadata_key=\"window\")]\n",
    "\n",
    "sentence_window_index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=sentence_window_vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )\n",
    "\n",
    "sentence_window_query_engine = create_query_engine(\n",
    "    index=sentence_window_index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )\n",
    "\n",
    "sentence_window_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setup Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "sentence_window_chain = [input_component, sentence_window_query_engine]\n",
    "\n",
    "sentence_window_query_pipeline = create_query_pipeline(sentence_window_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_query_pipeline.run(input=\"How can I effectively build strength across multiple facets of real life without relying on complicated machines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_query_pipeline.run(input=\"How can I set rules and speak honestly without worrying about hurting someone's feelings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë®‚Äçüë¶ Smaller Child Chunks Referring to Bigger Parent Chunk\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/0*x4rMd50GP99OSDuo.png\"  width=\"70%\">\n",
    "\n",
    "Source: [Ivan Ilin](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n",
    "\n",
    "üîó **Chunk References Explained:**\n",
    "\n",
    "- üß© **Concept**: Chunk References involve smaller chunks of data pointing to larger parent chunks, forming a hierarchical graph structure.\n",
    "  \n",
    "- üåê **Purpose**: This method is utilized in recursive retrieval to efficiently manage and access data in a structured manner.\n",
    "\n",
    "### Process During Query\n",
    "\n",
    "- üîç **During Query-Time**:\n",
    "\n",
    "  - **Small Chunk Retrieval**: Initially, smaller chunks relevant to the query are retrieved.\n",
    "\n",
    "  - **Following References**: The system then follows references to retrieve the larger parent chunks associated with these smaller chunks.\n",
    "\n",
    "- üìà **Benefits of Contextual Retrieval**:\n",
    "\n",
    "  - **Enhanced Context**: Retrieving larger chunks along with the smaller ones provides additional context.\n",
    "  \n",
    "  - **Improved Responses**: This deeper context allows for more accurate and comprehensive responses to queries.\n",
    "\n",
    "This structured approach ensures that data retrieval is both efficient and context-rich, enhancing the overall synthesis and response accuracy.\n",
    "\n",
    " The code below is creating a system where smaller chunks of text refer to the larger chunks they were created from. This allows for more context to be provided when retrieving chunks of text based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentenceSplitter class from the llama_index.core.node_parser module\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "# Define the sizes of chunks for sentence splitting\n",
    "sub_chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# Create a list of SentenceSplitter instances with different chunk sizes\n",
    "sub_node_parsers = [SentenceSplitter(chunk_size=c, chunk_overlap=16) for c in sub_chunk_sizes]\n",
    "\n",
    "# Initialize an empty list to store all index nodes\n",
    "all_nodes = []\n",
    "\n",
    "# Iterate over each base node in senpai_documents\n",
    "for base_node in senpai_documents:\n",
    "    # Process each base node with every SentenceSplitter in the list\n",
    "    for n in sub_node_parsers:\n",
    "        # Get sub-nodes by splitting the base node document into smaller parts\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        # Convert each sub-node into an IndexNode and link it to the base node's ID\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        # Add the newly created index nodes to the all_nodes list\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # Also add the original base node to the list of all nodes as an IndexNode\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_dict = {n.node_id: n for n in all_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes[5].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è  Ingest to Qdrant and Build the Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-small-to-big-parent-child\"\n",
    "\n",
    "parent_child_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "transforms = [Settings.embed_model]\n",
    "\n",
    "parent_child_nodes = ingest(\n",
    "    documents=all_nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=parent_child_vector_store\n",
    ")\n",
    "\n",
    "parent_child_index = create_index(\n",
    "    from_where=\"vector_store\", \n",
    "    embed_model=Settings.embed_model,\n",
    "    vector_store=parent_child_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Setup Query Engine for Parent Child Chunks\n",
    "\n",
    "We're making use of the `RecursiveRetriever` and the `RetrieverQueryEngine`.\n",
    "\n",
    "`RecursiveRetriever` is a separate class that is not directly associated with an index. It is uses multiple retrievers and query engines to recursively retrieve and query nodes.\n",
    "\n",
    "But, we can't directly use a `RecursiveRetriever` with the `index.as_retriever()` pattern we've seen before. The `index.as_retriever()` pattern is used to create a retriever from an index, and the type of retriever it creates depends on the `retriever_mode` argument you pass to it. `RecursiveRetriever` requires a dictionary of retrievers, and optionally a dictionary of query engines and a dictionary of node. There aren't required when creating a retriever using `index.as_retriever()`.\n",
    "\n",
    "So, we need to build the `RecursiveRetriever` and the `RetrieverQueryEngine` to accomplish this.\n",
    "\n",
    "  - `RecursiveRetriever` queries a graph of retrievers and query engines, following links between them to fetch relevant information for a given query. It recursively traverses the graph, deduplicates nodes, and returns the retrieved nodes along with any additional source nodes.\n",
    "\n",
    "  - `RetrieverQueryEngine` is a component that uses a retriever to fetch relevant documents or nodes based on a given query and then *synthesizes a response from the retrieved nodes using a `ResponseSynthesizer`*. It retrieves relevant nodes, applies postprocessing, synthesizes a response, and returns the result.\n",
    "\n",
    "\n",
    "More details are below.\n",
    "\n",
    "#### [`RecursiveRetriever`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/recursive_retriever.py)\n",
    "\n",
    "##### üåø Initialization\n",
    "\n",
    "   - Takes a root ID, retriever dict, and optional query engine & node dicts\n",
    "\n",
    "   - Validates root ID and checks for overlapping keys\n",
    "\n",
    "##### üîÑ Recursive Retrieval\n",
    "\n",
    "   - Starts from the root ID when `retrieve` is called with a query bundle\n",
    "\n",
    "   - Fetches the object (retriever, query engine, or node) for the current ID\n",
    "\n",
    "   - If it's a node, adds it to the list of nodes to return\n",
    "\n",
    "   - If it's a retriever, retrieves nodes and recursively queries them\n",
    "\n",
    "   - If it's a query engine, queries it and adds the response as a text node\n",
    "\n",
    "##### üîó Querying Retrieved Nodes\n",
    "\n",
    "   - For each retrieved IndexNode, recursively retrieves from the referenced ID\n",
    "\n",
    "   - For each TextNode, simply adds it to the list of nodes to return\n",
    "\n",
    "   - Avoids querying the same ID multiple times\n",
    "\n",
    "##### üßπ Deduplication\n",
    "\n",
    "   - Deduplicates nodes based on their node ID\n",
    "\n",
    "   - Keeps the node with the highest score or the first one returned\n",
    "\n",
    "##### üìö Retrieving All Nodes\n",
    "\n",
    "   - `retrieve_all` method retrieves all nodes, including additional source nodes\n",
    "\n",
    "   - Calls the recursive retrieval process and returns both retrieved and additional nodes\n",
    "\n",
    "\n",
    "#### [`RetrieverQueryEngine`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/retriever_query_engine.py)\n",
    "\n",
    "The `index.as_retriever()` pattern creates a retriever from an index. The type of retriever it creates depends on the `retriever_mode` argument you pass to it. \n",
    "\n",
    "So, while `index.as_retriever()` is used to create a retriever from an index, a `RetrieverQueryEngine` uses a retriever to fetch relevant nodes and a `ResponseSynthesizer` to synthesize a response.\n",
    "\n",
    "##### üåø Initialization\n",
    "   - Takes a retriever, an optional response synthesizer, and node postprocessors\n",
    "\n",
    "   - Creates a default response synthesizer if not provided\n",
    "   \n",
    "   - Sets up callback manager for the query engine and node postprocessors\n",
    "\n",
    "##### üîß Customization\n",
    "\n",
    "   - Can be initialized with various arguments using the `from_args` method\n",
    "\n",
    "   - Allows customization of response mode, prompt templates, async usage, etc.\n",
    "\n",
    "##### üîÑ Retrieval\n",
    "\n",
    "   - Retrieves nodes using the provided retriever when `retrieve` or `aretrieve` is called\n",
    "\n",
    "   - Applies node postprocessors to the retrieved nodes\n",
    "\n",
    "   - Returns the processed nodes\n",
    "\n",
    "##### üîÄ Retriever Swapping\n",
    "\n",
    "   - Allows swapping the retriever using the `with_retriever` method\n",
    "\n",
    "   - Creates a new RetrieverQueryEngine instance with the new retriever\n",
    "\n",
    "##### üß© Node Postprocessing\n",
    "   \n",
    "   - Applies a list of node postprocessors to the retrieved nodes\n",
    "\n",
    "   - Postprocessors can modify or filter the nodes based on the query bundle\n",
    "\n",
    "##### üé® Response Synthesis\n",
    "\n",
    "   - Synthesizes a response using the response synthesizer\n",
    "   - Takes the query bundle, retrieved nodes, and additional source nodes as input\n",
    "   - Generates a response based on the configured response mode and templates\n",
    "\n",
    "##### ‚ùì Querying\n",
    "\n",
    "   - Handles a query using the `_query` or `_aquery` method\n",
    "\n",
    "   - Retrieves nodes, synthesizes a response, and returns the response\n",
    "\n",
    "   - Triggers callback events for query start and end\n",
    "\n",
    "##### üèÉ Async Support\n",
    "\n",
    "   - Provides async versions of retrieval, synthesis, and querying methods\n",
    "\n",
    "   - Allows for asynchronous processing of queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "parent_child_retriever = parent_child_index.as_retriever(\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    )\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": parent_child_retriever},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "parent_child_query_engine = RetrieverQueryEngine.from_args(retriever_chunk, llm=Settings.llm)\n",
    "\n",
    "parent_child_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîßSetup Query Pipline for Parent Child Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "parent_child_chain = [input_component, parent_child_query_engine]\n",
    "\n",
    "parent_child_query_pipeline = create_query_pipeline(parent_child_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child_query_pipeline.run(input=\"How can I effectively build strength across multiple facets of real life without relying on complicated machines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child_query_pipeline.run(input=\"How can I set rules and speak honestly without worrying about hurting someone's feelings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
