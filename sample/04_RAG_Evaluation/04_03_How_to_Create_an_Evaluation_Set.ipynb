{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 datasets llama-index-embeddings-openai llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should install the following packages to your environment:\n",
    "\n",
    "`pip install datasets`\n",
    "\n",
    "`pip install llama-index-embeddings-fastembed`\n",
    "\n",
    "`pip install llama-index-llms-mistralai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using OpenAI here because Cohere has rate limits for it's free tier. You don't need to run this code yourself if you don't want to incur costs from OpenAI. I'll upload the dataset to the Hugging Face Hub and I'll show you how to download it from there when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already cleaned up our data before. Recall that we've persisted the `Document` objects to disk using a Docstore in such a way that each Document object represents cleaned text from a page of a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a set of `Documents` for the evaluation set\n",
    "\n",
    "- üìö **`group_documents_by_author`**: A utility function that sorts a collection of douments into groups based on who wrote them.\n",
    "\n",
    "- üóÇÔ∏è **How It Works**: It creates a  dictionary where each author's name is linked to all the documents they've written.\n",
    "  - Starts with an empty dictionary ready to be filled with author-document pairs.\n",
    "  - Goes through each document, checking the author's name and adding the document under the appropriate author in the dictionary.\n",
    "  - If a document doesn't list an author, it skips adding that document with a warning note.\n",
    "\n",
    "- üìù **Input**: Takes a list of `Document` objects, each with metadata that includes the `author` field (the name of its author).\n",
    "\n",
    "- üîñ **Output**: Outputs a dictionary that groups all the documents by their respective authors.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import group_documents_by_author\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìö **`sample_documents`**: Picks a set number of documents randomly from each author's collection within a grouped dictionary.\n",
    "\n",
    "- üé≤ **Sampling Logic**: It tries to get a specific number of documents for each author. If an author doesn't have enough documents, it alerts you.\n",
    "  - Begins with an empty list for storing selected samples.\n",
    "  - Loops through each author, considers only docs with >500 characters, checking if there are enough documents to fulfill the sampling requirement.\n",
    "  - Randomly selects the desired number of documents from those available, adding them to the overall sample list.\n",
    "  - Issues a warning if the documents under an author are too few to meet the sampling number.\n",
    "\n",
    "- üìù **Input**: Receives a dictionary where authors are keys and values are lists of their documents, along with an optional number of documents to sample per author.\n",
    "\n",
    "- üîñ **Output**: Outputs a list of randomly chosen documents from across all authors, sticking to the specified number per author when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sample_documents\n",
    "\n",
    "docs_for_eval_set = sample_documents(documents_by_author, num_samples=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_documents_by_author(documents):\n",
    "    \"\"\"\n",
    "    Count the number of documents each author has in a list of document objects.\n",
    "\n",
    "    :param documents: List of document objects with metadata containing 'author'.\n",
    "    :return: A Counter object with authors as keys and counts of their documents as values.\n",
    "    \"\"\"\n",
    "    # Extract the author from each document's metadata and count occurrences\n",
    "    author_counts = Counter(doc.metadata['author'] for doc in documents if 'author' in doc.metadata)\n",
    "    return author_counts\n",
    "\n",
    "author_counts = count_documents_by_author(docs_for_eval_set)\n",
    "for author, count in author_counts.items():\n",
    "    print(f\"Author '{author}' has {count} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_for_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest \n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size = 256,\n",
    "    chunk_overlap = 32\n",
    ")\n",
    "\n",
    "transformations = [splitter]\n",
    "\n",
    "docs_for_eval_set = ingest(documents = docs_for_eval_set, transformations = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_for_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create an evaluation set using custom prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from prompts import QUESTION_GEN_PROMPT\n",
    "print(QUESTION_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GEN_PROMPT_TEMPLATE = PromptTemplate(QUESTION_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "prompt = QUESTION_GEN_PROMPT_TEMPLATE.format(context_str=docs_for_eval_set[10].get_content()) \n",
    "\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ + ‚ùìGenerate questions from context\n",
    "\n",
    "We'll use GPT-3.5-Turbo to generate questions from our `Documents`\n",
    "\n",
    "Here's what the function below is doing:\n",
    "\n",
    "- Initialize an empty dictionary results to store the responses and contexts.\n",
    "\n",
    "- Iterate through each document doc in `docs_for_eval_set`.\n",
    "\n",
    "- For each document, we generate the prompt using `QUESTION_GEN_PROMPT_TEMPLATE` and the document's content.\n",
    "\n",
    "- Get the response from the LLM using `question_gen_llm.complete(prompt)`.\n",
    "\n",
    "- Store the response as the key, and the document's content as the value with the key \"context\" in the results dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "questions = []\n",
    "\n",
    "for doc in docs_for_eval_set:\n",
    "    result_dict = {}\n",
    "    context = doc.get_content()\n",
    "    prompt = QUESTION_GEN_PROMPT_TEMPLATE.format(context_str=context)\n",
    "    response = llm.complete(prompt)\n",
    "    result_dict['question'] = response.text\n",
    "    result_dict[\"context\"] =  context\n",
    "    questions.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ + üí¨ Create answers using generated question and context\n",
    "\n",
    "Using GPT-3.5-Turbo (to keep costs down, you can of course use GPT-4-Turbo), we'll generate answers using the questions we just created plus the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ANSWER_GEN_PROMPT\n",
    "\n",
    "print(ANSWER_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(ANSWER_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ANSWER_GEN_PROMPT_TEMPLATE.format(query_str=questions[42]['question'], context_str=questions[42]['context']) \n",
    "\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    prompt = ANSWER_GEN_PROMPT_TEMPLATE.format(query_str=question['question'], context_str=question['context']) \n",
    "    response = llm.complete(prompt)\n",
    "    question['answer'] = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßê How good are our questions?\n",
    "\n",
    "I suppose you could do this part before generating answers, if you wanted to...But we'll do it now.\n",
    "\n",
    "Here we're going to use GPT-4-Turbo to judge how good the questions is based on the context. We'll write a prompt that does this and score each question on a scale of 1-5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import GROUNDEDNESS_PROMPT\n",
    "\n",
    "print(GROUNDEDNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUNDEDNESS_PROMPT_TEMPLATE = PromptTemplate(GROUNDEDNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = GROUNDEDNESS_PROMPT_TEMPLATE.format(query_str=questions[42]['question'], context_str=questions[42]['context']) \n",
    "\n",
    "response = critic_llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    prompt = GROUNDEDNESS_PROMPT_TEMPLATE.format(query_str=question['question'], context_str=question['context']) \n",
    "    response = critic_llm.complete(prompt)\n",
    "    response_string = response.text\n",
    "    try:\n",
    "        score_as_int = int(response_string.split(\"Total rating: \")[-1].strip())\n",
    "        score_rational = response_string.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1]\n",
    "        question['question_groundedness_score'] = score_as_int\n",
    "        question['question_groundedness_score_rationale'] = score_rational\n",
    "    except Exception as e:\n",
    "        question['question_groundedness_score'] = None\n",
    "        question['question_groundedness_score_rationale'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "rag_eval_set = Dataset.from_list(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_set.push_to_hub(\"harpreetsahota/LI_Learning_RAG_Eval_Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can find the dataset on Hugging Face\n",
    "\n",
    "You don't have to run the examples here if you don't want to incur costs from OpenAI. \n",
    "\n",
    "[Here's the dataset](https://huggingface.co/datasets/harpreetsahota/LI_Learning_RAG_Eval_Set). You can click around and explore using the dataset viewer. If you sign-up for an account on Hugging Face, feel free to [follow me](https://huggingface.co/harpreetsahota)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rag_eval_set = load_dataset(\"harpreetsahota/LI_Learning_RAG_Eval_Set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
