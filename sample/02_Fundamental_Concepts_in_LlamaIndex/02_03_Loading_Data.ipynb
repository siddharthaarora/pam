{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ **Loading Data**\n",
    "\n",
    "Preparing your data for an LLM involves an ingestion pipeline similar to ML data cleaning or traditional ETL processes.\n",
    "\n",
    "### **Ingestion Pipeline Stages**\n",
    "  - üì• Load the data\n",
    "  - üîß Transform the data\n",
    "  - üóÉÔ∏è Index and store the data\n",
    "\n",
    "\n",
    "Let's start by downloading some example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Base URL for Project Gutenberg texts\n",
    "base_url = \"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "\n",
    "# Directory to save the downloaded files\n",
    "directory = Path(\"../data\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate a list of book IDs to download\n",
    "book_ids = range(1, 11)  # This will create a range from 1 to 10\n",
    "\n",
    "# Generate URLs for each book ID\n",
    "urls = [base_url.format(book_id=book_id) for book_id in book_ids]\n",
    "\n",
    "# Download each file and save it in the specified directory\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the filename from the URL using the book ID and create a file name\n",
    "        book_id = url.split('/')[-2]  # Extracts the book ID from the URL\n",
    "        filename = f\"pg{book_id}.txt\"\n",
    "        file_path = directory / filename\n",
    "        # Save the file to the specified directory\n",
    "        file_path.write_text(response.text)\n",
    "        print(f\"Downloaded {filename} to {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}. HTTP status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Load the data\n",
    "\n",
    "To use data with an LLM, first load it using data connectors, known as `Readers` in LlamaIndex, which format data into `Document` objects containing data and metadata.\n",
    "\n",
    "üìö **SimpleDirectoryReader**:\n",
    "  - The most straightforward loader is `SimpleDirectoryReader``.\n",
    "  - Built into LlamaIndex, it reads various formats (Markdown, PDFs, Word documents, PowerPoint decks, images, audio, video) from every file in a directory, creating documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[3].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manually Create Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "manual_document = Document(text=\"This is an example of a manual document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_document.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding metadata\n",
    "\n",
    "You can add metadata in the document constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_document_with_metadata = Document(\n",
    "    text=\"This is an example of a manual document\",\n",
    "    metadata={\"filename\": \"made-up-file-name\", \"category\": \"imaginary-category\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_document_with_metadata.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or after the document is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_document.metadata={\"filename\": \"made-up-file-name\", \"category\": \"imaginary-category\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_document.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Transform the data\n",
    "\n",
    "After loading, we must process and transform data for retrieval. We need to transform the list of `Document` objects into `Node` objects \n",
    "\n",
    "- ‚úÇÔ∏è Include chunking, extracting metadata, and embedding each chunk in transformations.\n",
    "\n",
    "- üåü Nodes are a first-class citizen in LlamaIndex, allowing direct definition or parsing from Documents.\n",
    "\n",
    "- üîÑ Transformation inputs and outputs are `Node` objects (Note: `Document` is subclass of `Node`).\n",
    "\n",
    "- üõ†Ô∏è Nodes are \"chunks\" of Documents, including text, images, etc., plus metadata and relationships.\n",
    "\n",
    "- üìä `NodeParser` classes convert Documents into Nodes with all necessary attributes. There are [a number of](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html) `NodeParser`'s you can choose from!\n",
    "\n",
    "- üìë High-level API: Use `.from_documents()` for automatic parsing and chunking of Document objects.\n",
    "\n",
    "- üîç Underlying process splits Document into Node objects, maintaining text and metadata with a link to their parent Document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(\n",
    "    chunk_size=128, # in tokens\n",
    "    chunk_overlap=16, #in tokens\n",
    "    paragraph_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nodes[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[42].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to construct Node objects manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "node1 = TextNode(text=\"Dad is married to Mom\", id_=\"001\")\n",
    "\n",
    "node2 = TextNode(text=\"Dad is Son's dad\", id_=\"002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NodeRelationships\n",
    "\n",
    "You can set relationships between nodes.\n",
    "\n",
    "- üåê NodeRelationships assign connections between chunks of text. It's useful for:\n",
    "  - Documents organized in a hierarchical manner (e.g., book, chapter, section, subsection)\n",
    "  - Maintaining sequential order\n",
    "  - Other complex relationships (ie, in legal documents for links a clause or other cases) \n",
    "\n",
    "- üîç NodeRelationships help retrieve not just the relevant section, but also related sections that might provide additional context or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n",
    "    node_id=node2.node_id\n",
    ")\n",
    "\n",
    "node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(\n",
    "    node_id=node1.node_id\n",
    ")\n",
    "nodes = [node1, node2]\n",
    "\n",
    "node2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(\n",
    "    node_id=node1.node_id, metadata={\"Romie\": \"Mom\", \"Harpreet\": \"Dad\", \"Jind\":\"Daughter\", \"Jugaad\":\"Son\"}, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of clean up, let's just go ahead and delete the text files we downloaded since we won't need them going forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
