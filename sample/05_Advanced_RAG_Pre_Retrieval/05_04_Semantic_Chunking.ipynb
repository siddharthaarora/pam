{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OPENAI_API_KEY key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    model=\"gpt-4o\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "import random\n",
    "\n",
    "all_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "senpai_documents = random.sample(all_documents, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Semantic Chunking\n",
    "\n",
    "This is a recent method that's been popularized by Greg Kamradt. He discussed this in detail in [an informative YouTube video](https://youtu.be/8OJC21T2SL4), which is also a great resource for more information on various chunking strategies.\n",
    "\n",
    "\n",
    "Here's the gist of what semantic chunking does:\n",
    "\n",
    "- Uses sentence embeddings to find breakpoints based on semantic similarity\n",
    "\n",
    "- Keeps related sentences together in the same chunk\n",
    "\n",
    "- Dynamically determines chunk size, no fixed length needed\n",
    "\n",
    "\n",
    "## How semantic chunking works\n",
    "\n",
    "1. ‚úÇÔ∏è Split document into sentences\n",
    "\n",
    "2. üî¢ Index sentences by position\n",
    "\n",
    "3. üéöÔ∏è Choose buffer size (sentences on either side to keep)\n",
    "\n",
    "4. üìä Measure similarity in embedding space\n",
    "   - Keep similar sentences together\n",
    "   - Split dissimilar sentences apart\n",
    "\n",
    "5. üß© Merge groups based on similarity threshold\n",
    "\n",
    "\n",
    "## [`SemanticSplitterNodeParser`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py)\n",
    "\n",
    "SemanticSplitterNodeParser a document into semantically related chunks called nodes. It uses semantic similarity and adaptive breakpoint determination, to create meaningful and coherent nodes from a document. \n",
    "\n",
    "### How it works\n",
    "\n",
    "#### 1. üß© Document Splitting\n",
    "\n",
    "   - The parser takes a document as input.\n",
    "\n",
    "   - It splits the document into individual sentences using a sentence splitter (e.g., split_by_sentence_tokenizer).\n",
    "\n",
    "#### 2. üéöÔ∏è Sentence Grouping\n",
    "\n",
    "   - The parser groups adjacent sentences together based on a configurable buffer size.\n",
    "\n",
    "   - The buffer size determines how many sentences are considered together when evaluating semantic similarity.\n",
    "\n",
    "   - For example, if the buffer size is 1, each sentence is treated individually. If it's greater than 1, sentences are grouped together.\n",
    "\n",
    "#### 3. üåê Embedding Calculation\n",
    "\n",
    "   - The parser calculates embeddings for each group of sentences using an embedding model.\n",
    "\n",
    "   - Embeddings represent the semantic meaning of the sentence groups in a dense vector format.\n",
    "\n",
    "#### 4. üìè Distance Calculation\n",
    "\n",
    "   - The parser calculates the cosine similarity between the embeddings of adjacent sentence groups.\n",
    "\n",
    "   - It then computes the distance by subtracting the similarity from 1.\n",
    "\n",
    "   - These distances represent the semantic dissimilarity between sentence groups.\n",
    "\n",
    "#### 5. üéØ Breakpoint Determination\n",
    "\n",
    "   - The parser determines breakpoints based on a configurable percentile threshold (e.g., 95th percentile).\n",
    "\n",
    "   - If the distance between two adjacent sentence groups exceeds the breakpoint threshold, it indicates a semantic shift and marks the start of a new node.\n",
    "\n",
    "#### 6. üß© Node Creation\n",
    "\n",
    "   - The parser splits the document into nodes based on the determined breakpoints.\n",
    "\n",
    "   - Each node represents a semantically related chunk of text.\n",
    "\n",
    "   - The sentences within a node are combined to form a coherent unit of information.\n",
    "\n",
    "#### 7. üìù Node Metadata\n",
    "\n",
    "   - The parser can include additional metadata in the nodes, such as the original text or other relevant information.\n",
    "\n",
    "   - It can also establish relationships between nodes, such as previous and next relationships, to maintain the sequential order of the chunks.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "- `embed_model`: The embedding model to use for semantic comparison. If not provided, the parser will attempt to use the OpenAIEmbedding model. If the `llama-index-embeddings-openai` package is not installed, an ImportError will be raised.\n",
    "\n",
    "- `buffer_size`: The number of sentences to group together when evaluating semantic similarity. \n",
    "\n",
    "  - Default value is 1 (each sentence is considered individually). \n",
    "  \n",
    "  - Increasing the buffer size allows the parser to consider the context of adjacent sentences when determining semantic similarity. This can help capture more meaningful relationships between sentences.\n",
    "\n",
    "- `breakpoint_percentile_threshold`: The percentile of cosine dissimilarity that must be exceeded between a group of sentences and the next to form a node. \n",
    "\n",
    "  - A smaller value results in more nodes being generated. Default value is 95 (95th percentile). \n",
    "\n",
    "  - Adjusting this threshold allows you to control the granularity of the node splits. A lower value will create more nodes, while a higher value will create fewer nodes.\n",
    "\n",
    "- `sentence_splitter`: The function or callable object used to split the text into sentences. Default is `split_by_sentence_tokenizer` (again, using the `PunktSentenceTokenizer` from the `nltk` library). The choice of sentence splitter can affect how the text is divided into individual sentences, which in turn influences the node splitting process.\n",
    "\n",
    "  - While the `sentence_splitter` is used to initially split the document into individual sentences, the actual determination of node boundaries is based on semantic similarity rather than a fixed splitting approach.\n",
    "  \n",
    "  - `sentence_splitter` is more of a preprocessing step to prepare the document for the semantic analysis. It ensures that the parser has a consistent input format to work with (i.e., a list of sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "\n",
    "def semantic_splitter(\n",
    "    embed_model,\n",
    "    buffer_size, \n",
    "    breakpoint_percentile_threshold, \n",
    "    documents,\n",
    "    **kwargs):\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "        embed_model=embed_model,\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You have a lot of design choices to make here\n",
    "\n",
    "These are all points of experimentation for you. Hack around with these, run evaluation against the metrics that matter to you, vibe check the results, and find one that works best.\n",
    "\n",
    "- What embedding model do you want to use?\n",
    "\n",
    "- What's the dimensions you want to use for the embedding model?\n",
    "\n",
    "- What buffer size do you want to use? \n",
    "\n",
    "- What about the breakpoint threshold?\n",
    "\n",
    "- What sentence splitter do you want to use?\n",
    "\n",
    "I'll use some arbitrary settings for illustrative purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_nodes = semantic_splitter(\n",
    "    embed_model = Settings.embed_model,\n",
    "    buffer_size = 3, \n",
    "    breakpoint_percentile_threshold = 0.55, \n",
    "    documents = senpai_documents\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semantic_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_nodes[101].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_nodes[100].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è Build the Index and Ingest to Qdrant\n",
    "\n",
    "Note: This will also take a long time (about 30 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-semantic-nodes\"\n",
    "\n",
    "semantic_nodes_vector_store = setup_vector_store(\":memory:\", QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [Settings.embed_model]\n",
    "\n",
    "semantic_nodes = ingest(\n",
    "    documents=semantic_nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=semantic_nodes_vector_store\n",
    ")\n",
    "\n",
    "semantic_nodes_index = create_index(\n",
    "    from_where=\"vector_store\", \n",
    "    embed_model=Settings.embed_model,\n",
    "    vector_store=semantic_nodes_vector_store\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "from utils import create_query_engine\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "semantic_nodes_query_engine = create_query_engine(\n",
    "    index=semantic_nodes_index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    text_qa_template=HYPE_ANSWER_GEN_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setup Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "semantic_nodes_chain = [input_component,  semantic_nodes_query_engine]\n",
    "\n",
    "semantic_nodes_query_pipeline = create_query_pipeline(semantic_nodes_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_nodes_query_pipeline.run(input=\"How can I navigate the maze of the market while building a company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
