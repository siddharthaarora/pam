{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    model=\"gpt-4o\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(provider=\"openai\", model=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"prompt-compression\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Compression using LongLLMLingua\n",
    "\n",
    "[LongLLMLingua](https://llmlingua.com/) is a method for improving the performance and efficiency of RAG and long context scenarios by compressing prompts. It addresses issues such as the \"lost in the middle\" problem, high costs, and context window limitations in RAG.\n",
    "\n",
    "<img src=\"https://llmlingua.com/videos/figures/motivation.png\" style=\"width:80%; height:auto;\">\n",
    "\n",
    "Image Source: [LongLLMLingua Project](https://llmlingua.com/)\n",
    "\n",
    "The key components of LongLLMLingua are:\n",
    "\n",
    "1. Question-aware Coarse-Grained prompt compression: Evaluates the relevance between the context and the question based on the perplexity corresponding to the question.\n",
    "\n",
    "2. Question-aware Fine-grained Prompt Compression: Uses contrastive perplexity to extract key tokens from documents that are relevant to the question.\n",
    "\n",
    "3. Adaptive granular control during compression: Dynamically allocates different compression ratios to different documents based on the rank information obtained from the coarse-grained compression.\n",
    "\n",
    "4. Subsequence recovery: Recovers the original prompt content by establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt.\n",
    "\n",
    "Experiments show that LongLLMLingua can improve performance by up to 21.4 points at a 4x compression rate in RAG scenarios, and it outperforms retrieval-based and compression-based methods in long context benchmarks like LongBench and ZeroScrolls.\n",
    "\n",
    "<img src=\"https://llmlingua.com/videos/figures/LLMLingua.png\"  style=\"width:80%; height:auto;\">\n",
    "\n",
    "\n",
    "# [`LongLLMLinguaPostprocessor`](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/postprocessor/llama-index-postprocessor-longllmlingua/llama_index/postprocessor/longllmlingua/base.py)\n",
    "\n",
    "The `LongLLMLinguaPostprocessor` is a postprocessor that optimizes the nodes by compressing the context using the LongLLMLingua method described in the paper. It aims to shorten the node text based on the given query to improve efficiency and reduce computational costs.\n",
    "\n",
    "**Arguments you need to know:**\n",
    "\n",
    "- `model_name`: The name of the pre-trained language model to use for compression (default: `\"NousResearch/Llama-2-7b-hf\"`).\n",
    "\n",
    "- `device_map`: The device to use for running the model (default: `\"cuda\"`).\n",
    "\n",
    "- `model_config`: Additional configuration options for the language model (default: empty dictionary).\n",
    "\n",
    "- `open_api_config`: Configuration options for the OpenAI API, if used (default: empty dictionary).\n",
    "\n",
    "- `metadata_mode`: The mode for handling metadata during postprocessing (default: `MetadataMode.ALL`).\n",
    "\n",
    "- `instruction_str`: The instruction string to provide context for the compression (default: \"Given the context, please answer the final question\").\n",
    "\n",
    "- `target_token`: The target number of compressed tokens (default: 300).\n",
    "\n",
    "- `rank_method`: The ranking method to use for compression (default: \"longllmlingua\").\n",
    "\n",
    "- `additional_compress_kwargs`: Additional keyword arguments to pass to the compression method (default: empty dictionary).\n",
    "\n",
    "**What happens under the hood:**\n",
    "\n",
    "- Extracts the content of each node based on the `metadata_mode`.\n",
    "\n",
    "- Splits the context texts by \"\\n\\n\" to create a new list of context texts.\n",
    "\n",
    "- Calls the `compress_prompt` method of the `PromptCompressor` instance, passing the new context texts, instruction string, query string, target token count, ranking method, and additional compression keyword arguments.\n",
    "\n",
    "- The `compress_prompt` method returns a compressed prompt, which is then split by \"\\n\\n\" to separate the individual compressed context texts.\n",
    "\n",
    "- The question and instruction, which are appended to the top and bottom of the compressed prompt, are removed.\n",
    "\n",
    "- The remaining compressed context texts are used to create new `TextNode` instances, which are then wrapped in `NodeWithScore` objects and returned as the optimized nodes.\n",
    "\n",
    "\n",
    "The **question-aware fine-grained compression** feature is NOT implemented in LlamaIndex. The compression is primarily based on the ***coarse-grained approach*** and the ranking method specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.postprocessor.longllmlingua import LongLLMLinguaPostprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_postprocessor = LongLLMLinguaPostprocessor(\n",
    "    instruction_str=\"Given the context, please answer the final question\",\n",
    "    target_token=300,\n",
    "    rank_method=\"longllmlingua\",\n",
    "    additional_compress_kwargs={\n",
    "        \"condition_compare\": True,\n",
    "        \"condition_in_question\": \"after\",\n",
    "        \"context_budget\": \"+100\",\n",
    "        \"reorder_context\": \"sort\",  # enable document reorder\n",
    "        \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It  has the same usage as the other node postprocessors:\n",
    "\n",
    "\n",
    "```python\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.postprocessor import YourPostProcessOfChoice\n",
    "\n",
    "... # prior steps to this point: index is defined\n",
    "\n",
    "your_post_processor = YourPostProcessOfChoice(WhateverArgumentsYouNeedToPass)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    ..., #all your othe query engine arguments\n",
    "    node_postprocessors = your_post_processor\n",
    ")\n",
    "\n",
    "# for just a single query\n",
    "response = query_engine.query(\"your query\")\n",
    "\n",
    "# use the query pipeline and apply to your entire dataset if you'd like\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
