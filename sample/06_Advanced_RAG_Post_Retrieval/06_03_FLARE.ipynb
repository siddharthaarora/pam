{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    model=\"gpt-4o\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(provider=\"openai\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"flare\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü FLARE: Forward-Looking Active REtrieval augmented generation\n",
    "\n",
    "[Active Retrieval Augmented Generation](https://arxiv.org/pdf/2305.06983.pdf) is a promising approach to enhance the factual accuracy of LLMs by retrieving relevant information from external knowledge sources throughout the generation process. \n",
    "\n",
    "FLARE (Forward-Looking Active REtrieval augmented generation) is a novel ARAG method that actively decides when and what to retrieve, leading to improved performance in long-form knowledge-intensive generation tasks.\n",
    "\n",
    "#### üîç The Limitations of Single-Retrieval Approaches\n",
    "\n",
    "- LLMs often hallucinate and generate factually inaccurate output\n",
    "\n",
    "- Existing retrieval-augmented LMs mostly retrieve information only once based on the input\n",
    "\n",
    "- Single retrieval is insufficient for generating long texts, where continually gathering information is essential\n",
    "\n",
    "#### ‚ú® Actively Retrieving Information as Needed\n",
    "\n",
    "- FLARE iteratively predicts the upcoming sentence to anticipate future content\n",
    "\n",
    "- The predicted sentence is used as a query to retrieve relevant documents\n",
    "\n",
    "- If the predicted sentence contains low-confidence tokens, FLARE regenerates it using the retrieved documents\n",
    "\n",
    "- This process continues until the entire response is generated\n",
    "\n",
    "#### üéØ Two Variants of FLARE\n",
    "\n",
    "1. `FLAREinstruct`: Prompts the LM to generate retrieval queries when necessary using retrieval-encouraging instructions\n",
    "\n",
    "2. `FLAREdirect`: Directly uses the LM's generated sentence as the retrieval query if it contains uncertain tokens\n",
    "\n",
    "####  üîé Confidence-Based Retrieval and Query Formulation\n",
    "\n",
    "- FLARE employs confidence-based active retrieval, triggering document retrieval only when the LM lacks necessary knowledge\n",
    "\n",
    "- Confidence-based query formulation methods include using masked sentences as implicit queries and generating questions as explicit queries\n",
    "\n",
    "The provided code defines a class called `FLAREInstructQueryEngine` which is a query engine based on the FLARE (Active Retrieval Augmented Generation) paper.\n",
    "\n",
    "# [`FLAREInstructQueryEngine`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/flare/base.py)\n",
    "\n",
    "Combines retrieval and generation capabilities to generate responses based on the FLARE approach, leveraging retrieval-encouraging instructions and iterative refinement.\n",
    "\n",
    "## Arguments you need to know\n",
    "\n",
    "- `query_engine`: The underlying query engine to use for retrieval.\n",
    "\n",
    "- `llm` (optional): The language model to use for generating responses.\n",
    "\n",
    "- `instruct_prompt` (optional): The prompt template for generating instructions.\n",
    "\n",
    "- `lookahead_answer_inserter` (optional): The component for inserting lookahead answers.\n",
    "\n",
    "- `done_output_parser` (optional): The parser for determining if the response is complete.\n",
    "\n",
    "- `query_task_output_parser` (optional): The parser for extracting query tasks from the response.\n",
    "\n",
    "- `max_iterations` (optional): The maximum number of iterations for generating the response.\n",
    "\n",
    "- `max_lookahead_query_tasks` (optional): The maximum number of query tasks to consider for lookahead.\n",
    "\n",
    "\n",
    "### Under the hood\n",
    "\n",
    "The key idea behind the `FLAREInstructQueryEngine` is to break down the query answering process into smaller steps. Instead of generating a complete response in one shot, it generates a partial response with placeholders, retrieves specific information to fill those placeholders, and iteratively refines the response.\n",
    "\n",
    "\n",
    "1. It receives a query from the user.\n",
    "\n",
    "2. It generates a \"lookahead response\" based on the query and the current state of the response. The lookahead response is a tentative response that includes placeholders for additional information to be retrieved.\n",
    "\n",
    "3. It analyzes the lookahead response to identify specific sub-queries or \"query tasks\" that need to be answered to complete the response.\n",
    "\n",
    "4. It sends these query tasks to an underlying query engine to retrieve relevant information from a knowledge base or corpus.\n",
    "\n",
    "5. It incorporates the retrieved information into the lookahead response, replacing the placeholders with the actual retrieved content.\n",
    "\n",
    "6. It updates the current response by appending the relevant parts of the updated lookahead response.\n",
    "\n",
    "7. It repeats steps 2-6 iteratively, refining the response with each iteration until a maximum number of iterations is reached or the response is considered complete.\n",
    "\n",
    "\n",
    "The \"instructions\" in the name refer to the prompts and templates used to guide the language model in generating the lookahead responses and identifying the query tasks. These instructions encourage the model to focus on retrieving relevant information rather than generating everything from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate query engine and FLARE query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import FLAREInstructQueryEngine\n",
    "\n",
    "index_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "flare_query_engine = FLAREInstructQueryEngine(\n",
    "    query_engine=index_query_engine,\n",
    "    max_iterations=7,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict\n",
    "\n",
    "display_prompt_dict(flare_query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "flare_chain = [input_component,  flare_query_engine]\n",
    "\n",
    "flare_query_pipeline = create_query_pipeline(flare_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_query_pipeline.run(input=\"How can I ensure that outward distractions do not interrupt my good thoughts and focus?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
