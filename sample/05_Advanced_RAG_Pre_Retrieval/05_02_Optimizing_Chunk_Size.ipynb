{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 openai==1.30.1 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OPENAI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Chunk Size\n",
    "\n",
    "In this lesson, we'll explore what chunking is, how it affects the indexing and retrieval process, and how you can customize chunk size and overlap to optimize your results.\n",
    "\n",
    "> **The Chunking Commandment:** Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\n",
    ">\n",
    "> -- Greg Kamradt, [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "## Understanding Chunking\n",
    "\n",
    "When documents are ingested into an index, `LlamaIndex` splits them into smaller pieces called \"chunks.\"  This process is known as chunking. By default, LlamaIndex uses a *chunk size* of 1024 and a *chunk overlap* of 20. \n",
    "\n",
    "But what do these numbers mean, and how do they impact the indexing and retrieval process?\n",
    "\n",
    "### Chunk Size\n",
    "\n",
    "The chunk size determines the maximum number of tokens (roughly equivalent to words) that each chunk will contain. With a default chunk size of 1024, `LlamaIndex` will split your documents into chunks that are no longer than 1024 tokens each.\n",
    "\n",
    "#### **🤏 Smaller Chunk Size**\n",
    "\n",
    "*   More precise and focused embeddings\n",
    "\n",
    "*   Beneficial for retrieving specific information\n",
    "\n",
    "#### **👐Larger Chunk Size**\n",
    "\n",
    "*   More general embeddings with broader context\n",
    "\n",
    "*   Useful for document overviews, but may miss details\n",
    "\n",
    "### Chunk Overlap\n",
    "\n",
    "*   Shared tokens between adjacent chunks (default: 20)\n",
    "\n",
    "*   Maintains context and prevents information loss\n",
    "\n",
    "I recommend taking a look at [this chunk visualizer](https://huggingface.co/spaces/m-ric/chunk_visualizer) to get an intuitive sense for chunk size and overlap.\n",
    "\n",
    "## 🤔 The Impact of Chunk Size\n",
    " \n",
    "I recommend reading [this blog](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) post by the LlamaIndex team.\n",
    "\n",
    " #### **📏 Relevance and Granularity**\n",
    "\n",
    "*   Smaller chunks (e.g., 128) offer granularity but risk missing vital information, or lack sufficient context.\n",
    "\n",
    "*   Larger chunks (e.g., 512) are more likely to capture necessary context, but also run the risk of including irrelevant information.\n",
    "\n",
    "*   Faithfulness and Relevancy metrics help assess response quality. \n",
    "\n",
    " #### **🎯 Chunk Size and Use Case**\n",
    "\n",
    "*   **Question Answering:** Shorter, specific chunks for precise answers.\n",
    "\n",
    "*   **Summarization:** Longer chunks to capture the overall context.\n",
    "\n",
    " #### **⏳ Response Generation Time**\n",
    "\n",
    "*   Larger chunks provide more context but may slow down the system.\n",
    "\n",
    "*   Balancing comprehensiveness with speed is crucial.\n",
    "    \n",
    " #### **⚖️ Finding the Optimal Size**\n",
    "\n",
    "*   Testing various chunk sizes is essential for specific use cases and datasets. \n",
    "\n",
    "*   Balancing information capture with efficiency is key.\n",
    "\n",
    "### Considerations When Customizing Chunk Size\n",
    "\n",
    "When deciding on a chunk size, there are a few things to keep in mind:\n",
    "\n",
    "| Factor | Description |\n",
    "|--------|-------------|\n",
    "| 📄 **Data Characteristics** | The optimal chunk size depends on the data you're indexing. Long, detailed documents, may require a larger chunk size to capture more context. Smaller chunk size may be more appropriate for short, focused passages. |\n",
    "| 🔍 **Retrieval Requirements** | If you need to retrieve very specific details, a smaller chunk size may be better. If you're looking for more general information, a larger chunk size may suffice. |\n",
    "| 🔢 **Similarity Parameters** | With a smaller chunk size, the embeddings become more specific, and as a result, there might be more relevant chunks that match a given query. To accommodate this increase in potentially relevant chunks, it is advisable to increase the `similarity_top_k` parameter. This adjustment ensures that the query engine does not overlook relevant results due to a too narrow top-k selection. |\n",
    "\n",
    "### There are [various methods](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules) you can use to chunk your documents. \n",
    "\n",
    "| Parser Type | Splitter Name | Description |\n",
    "|-------------|---------------|-------------|\n",
    "| 📁 File-Based Node Parsers | 📄`SimpleFileNodeParser` | The simplest flow: `FlatFileReader` + `SimpleFileNodeParser` which automatically use the best node parser for each type of content. Then, you may want to chain the file-based node parser with a text-based node parser to account for the actual length of the text. |\n",
    "| | 🌐`HTMLNodeParser` | This node parser uses beautifulsoup to parse raw HTML. By default, it will parse a select subset of HTML tags, but you can override this. The default tags are: [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"] |\n",
    "| | 🎭`JSONNodeParser` | The `JSONNodeParser` parses raw JSON. |\n",
    "| | 📝`MarkdownNodeParser` | The `MarkdownNodeParser` parses raw markdown text. |\n",
    "| ✂️ Text-Splitters | 💻`CodeSplitter` | Splits raw code-text based on the language it is written in. |\n",
    "| | 🦜🔗`LangchainNodeParser` | You can also wrap any existing text splitter from langchain with a node parser. |\n",
    "| | 📜`SentenceSplitter` | The `SentenceSplitter` attempts to split text while respecting the boundaries of sentences. |\n",
    "| | 🪟`SentenceWindowNodeParser` | The `SentenceWindowNodeParser` splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.|\n",
    "| | 🧠`SemanticSplitterNodeParser` | Instead of chunking text with a fixed chunk size, the semantic splitter adaptively picks the breakpoint in-between sentences using embedding similarity. This ensures that a \"chunk\" contains sentences that are semantically related to each other. |\n",
    "| | 🪙`TokenTextSplitter` | The `TokenTextSplitter` attempts to split to a consistent chunk size according to raw token counts. |\n",
    "| 🔗 Relation-Based Node Parsers | 🌿`HierarchicalNodeParser` | This node parser will chunk nodes into hierarchical nodes. This means a single input will be chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. |\n",
    "\n",
    "\n",
    "## We're only going to focus on a few strategies\n",
    "\n",
    "I'll show you how to split/chunk test using each method below. \n",
    "\n",
    "\n",
    " - 🪙`TokenTextSplitter`\n",
    " \n",
    " - 📜`SentenceSplitter`\n",
    "\n",
    "### We'll cover these in later lessons\n",
    " \n",
    " - 🪟`SentenceWindowNodeParser`\n",
    "\n",
    " - 🧠`SemanticSplitterNodeParser`\n",
    "\n",
    "# 🪙 [`TokenTextSplitter`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py)\n",
    "\n",
    "The primary function is to divide a given text into smaller chunks, ensuring each chunk stays within a specified token limit. \n",
    "\n",
    "### **How it Works**\n",
    "\n",
    "1.  **Tokenization:** It utilizes a tokenizer to break down the text into individual tokens (words or subwords).  The default tokenizer is the `tiktoken` tokenizer for GPT-3.5-Turbo.\n",
    "\n",
    "2.  **Chunking:** It then groups these tokens into chunks, ensuring each chunk's size is within the defined `chunk_size` limit. \n",
    "\n",
    "3.  **Overlap Handling:** To maintain context and coherence between chunks, it can incorporate an overlap, specified by `chunk_overlap`, where the last few tokens of one chunk are repeated at the beginning of the next.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`chunk_size`**: Controls the maximum token count for each chunk. Defualts to 1024.\n",
    "\n",
    "*   **`chunk_overlap`**: Determines the number of overlapping tokens between consecutive chunks. Defaults to 20.\n",
    "\n",
    "*   **`separator`**: Specifies the primary character used to split the text into words. Defaults to space (`\" \"`). \n",
    "\n",
    "*   **`backup_separators`**: Provides additional characters for splitting if the primary separator isn't sufficient. Defaults to new line character (`\"\\n\"`).\n",
    "\n",
    "Note: The order of splitting is: 1. split by separator, 2. split by backup separators (if any), 3. split by characters\n",
    "\n",
    "*   **`include_metadata`**: Enables or disables the inclusion of metadata within each chunk. Defaults to `True`.\n",
    "\n",
    "* **`include_prev_next_rel`**: Enables or disables tracking the relationship between nodes. Defaults to `True`.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "The basic usage pattern is as follows (you don't need to pass anything if you want to keep the default values.):\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter()\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "```\n",
    "I'll limit our exploration to the `chunk_sizes = [64, 128, 256, 512]` and hold `chunk_overlap` fixed to 16 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senpai_documents[42].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "example_split = TokenTextSplitter(chunk_size=64, chunk_overlap=16).split_text(senpai_documents[42].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_split[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str,encoding=encoding) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = encoding\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encoding.encode(example_split[0])\n",
    "\n",
    "for token in tokens:\n",
    "    print(encoding.decode_single_token_bytes(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_from_string(example_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "def token_splitter(chunk_size, documents):\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=16,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter_results = {}\n",
    "\n",
    "chunk_sizes = [64, 128, 256, 512]\n",
    "\n",
    "# Iterate over each chunk size and perform token splitting\n",
    "for size in chunk_sizes:\n",
    "    key = f\"token_split_chunk_size_{size}\"\n",
    "    token_splitter_results[key] = token_splitter(size, senpai_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in token_splitter_results.items():\n",
    "    print(f\"With {key} we get {len(value)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [📜`SentenceSplitter`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/sentence.py)\n",
    "\n",
    "The `SentenceSplitter` class, as its name suggests, specializes in splitting text while trying to keep complete sentences and paragraphs together. This is in contrast to the `TokenTextSplitter`, which focuses on token limits.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. **Initial Splitting**\n",
    "\n",
    "    *   The text is first divided into paragraphs using the specified `paragraph_separator` (defaults to triple newline characters `\"\\n\\n\\n\"`).\n",
    "\n",
    "    *   Each paragraph is then further split using a \"chunking tokenizer\" (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library). Which basically looks for sentences boundaries.\n",
    "\n",
    "    *   If these methods don't yield enough splits, it resorts to a backup regex and the default separators (`CHUNKING_REGEX = \"[^,.;。？！]+[,.;。？！]?\"`).\n",
    "\n",
    "2. **Chunking with Sentence Awareness**\n",
    "\n",
    "    *   The resulting splits are grouped into chunks, keeping sentences together as much as possible. \n",
    "\n",
    "    *   It considers the `is_sentence` flag for each split during this process.\n",
    "\n",
    "    *   Chunk size and overlap still play a role, but sentence boundaries are given preference.\n",
    "\n",
    "3. **Overlap Handling**\n",
    "\n",
    "    *   Similar to `TokenTextSplitter`, it incorporates overlap between chunks to maintain context. \n",
    "\n",
    "    *   However, it prioritizes using the last complete sentence for overlap rather than just the last few tokens.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`chunk_size`**: The target token size for each chunk.\n",
    "\n",
    "*   **`chunk_overlap`**: The number of overlapping tokens between chunks.\n",
    "\n",
    "*   **`separator`**: The default separator for splitting (e.g., space).\n",
    "\n",
    "*   **`paragraph_separator`**: The string used to identify paragraph breaks.\n",
    "\n",
    "*   **`secondary_chunking_regex`**: A backup regex for splitting if the primary methods are insufficient.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=50)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "### When to Use SentenceSplitter\n",
    "\n",
    "*   When preserving complete sentences and paragraphs is essential for understanding the context.\n",
    "\n",
    "*   When dealing with text where sentence boundaries are meaningful (e.g., legal documents, narratives).\n",
    "\n",
    "*   When you want to avoid having broken sentences at the beginning or end of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "SentenceSplitter(chunk_size=64, chunk_overlap=16).split_text(senpai_documents[42].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(chunk_size, documents):\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=16,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter_results = {}\n",
    "\n",
    "# Iterate over each chunk size and perform sentence splitting\n",
    "for size in chunk_sizes:\n",
    "    key = f\"sentence_split_chunk_size_{size}\"\n",
    "    sentence_splitter_results[key] = sentence_splitter(size, senpai_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sentence_splitter_results.items():\n",
    "    print(f\"With {key} we get {len(value)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: `TokenTextSplitter` vs `SentenceSplitter`\n",
    "\n",
    "`TokenTextSplitter` splits the text into chunks based on a specified number of tokens. It uses a tokenizer to break down the text into individual tokens (words or subwords), and then groups these tokens into chunks of a specified size. If the text doesn't divide evenly into the specified chunk size, the last chunk will contain the remaining tokens, which could be less than the specified chunk size.\n",
    "\n",
    "`SentenceSplitter`, on the other hand, splits the text into chunks based on sentences. It uses a sentence boundary detection algorithm to identify where sentences begin and end, and then groups these sentences into chunks. The size of these chunks can vary depending on the length of the sentences.\n",
    "\n",
    "# Select Strategy for Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "# Randomly select a key from the chunk_size_results dictionary\n",
    "strategies = list(token_splitter_results.keys()) + list(sentence_splitter_results.keys())\n",
    "random_key = random.choice(strategies)\n",
    "print(f\"Randomly selected key: {random_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"wots_sentence_split_chunk_size_256\"\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    temperature=0.75, \n",
    "    model=\"gpt-4o\", \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest\n",
    "\n",
    "# what splitter are we gonna use?\n",
    "sent_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "transforms = [sent_splitter, Settings.embed_model]\n",
    "\n",
    "split_nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index Over VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from utils import create_index\n",
    "\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\", \n",
    "    embed_model=Settings.embed_model,\n",
    "    vector_store=vector_store,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Query Engine\n",
    "\n",
    "The default response mode for the query engine is `refine` which will create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.\n",
    "\n",
    "I am changing the response mode to `compact`, which is similar to refine but it concatenate the chunks beforehand, resulting in less LLM calls. \n",
    "\n",
    "You can [visit the LlamaIndex docs](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/) to learn more about the choices you have here. Just note that this is also a hyperparameter that you have control over, which will also impact your generation. \n",
    "\n",
    "I will leave this up to you to hack around with.\n",
    "\n",
    "I'm also going to change the value of `similiarty_top_k` from it's default value of 2 to 5. This is an arbitrary choice and simple meant to illustrate that it's a hyperparameter under your control which will affect your generation results.  Increasing this value means you will increase your probability of fetching the most relevant documents from the vector database. \n",
    "\n",
    "#### Vector Store Query Mode\n",
    "\n",
    "The query engine also has a parameter for `vector_store_query_mode`, there are [several choices you can make here](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/vector_stores/types.py). \n",
    "\n",
    "`Maximal Marginal Relevance (MMR)` balances relevance and diversity when selecting a subset of items from a larger set.\n",
    "\n",
    "The key idea behind MMR is to iteratively select items that are both highly relevant to the query and also different from the items already selected. This is achieved by maximizing a score that combines two components:\n",
    "\n",
    "1. **Relevance**: The similarity between the item and the query (e.g. cosine similarity)\n",
    "\n",
    "2. **Diversity**: The maximum similarity between the item and any of the items already selected\n",
    "\n",
    "The MMR score is a linear combination of these components, controlled by a parameter `λ` (lambda) that determines the trade-off between relevance and diversity:\n",
    "\n",
    "`MMR = argmax [λ * Relevance(item, query) - (1-λ) * max(Similarity(item, selected_item))]`\n",
    "\n",
    "- If λ is close to 1, MMR puts more emphasis on relevance. \n",
    "\n",
    "- If λ is close to 0, MMR favors diversity.\n",
    "\n",
    "MMR can improve the retrieval component by selecting a diverse set of relevant passages. This helps capture different aspects of the query and provides the language model with a richer context for generating the final answer.\n",
    "\n",
    "The benefits of using MMR in RAG include:\n",
    "\n",
    "1. Avoid selecting passages that contain very similar information.\n",
    "\n",
    "2. Increases the chances of covering different facets of the query.\n",
    "\n",
    "3. Providing the language model with a diverse set of relevant passages can lead to more comprehensive and well-rounded answers.\n",
    "\n",
    "### I will leave it up to you to experiment with using MMR (or, not using it) as well as experimenting with different λ values.\n",
    "\n",
    "The pattern for how to use it is there for you to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_engine\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can't forget about the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ANSWER_GEN_PROMPT\n",
    "\n",
    "print(ANSWER_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(ANSWER_GEN_PROMPT)\n",
    "\n",
    "query_engine.update_prompts({'response_synthesizer:text_qa_template':ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline.run(input=\"How can I become the best in the world at what I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline.run(input=\"How can I build my brand and make a name for myself in order to be uniquely qualified for emerging opportunities in technology?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline.run(input=\"How can I set up systems to be the most successful version of myself while working the least hard possible?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
