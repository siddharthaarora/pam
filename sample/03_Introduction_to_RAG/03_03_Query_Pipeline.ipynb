{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pipelines\n",
    "\n",
    "<img src=\"https://docs.llamaindex.ai/en/stable/_static/query/pipeline_rag_example.png\">\n",
    "\n",
    "Source: [LlamaIndex Docs](https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/)\n",
    "\n",
    "LlamaIndex offers a query API for chaining modules to manage data workflows easily. It revolves around the QueryPipeline, where you link various modules like LLMs, prompts, and retrievers in a sequence or DAG for end-to-end execution.\n",
    "\n",
    "You can streamline workflows efficiently using QueryPipeline, reducing code complexity and enhancing readability. Additionally, a declarative interface ensures easy serialization of pipeline components for portability and deployment across systems in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = Cohere(model=\"command-r-plus\", api_key=CO_API_KEY)\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# Create a Qdrant client\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"it_can_be_done\"\n",
    "    )\n",
    "\n",
    "# Create a vector store index\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A RAG Pipeline with PromptTemplate\n",
    "\n",
    "I'm going to kick it off with a slightly complex workflow where the input is passes through two prompts before initiating retrieval.\n",
    "\n",
    "1. Retrieve question about given topic.\n",
    "\n",
    "2. Rephrase the context\n",
    "\n",
    "Each prompt only takes in one input, so `QueryPipeline` will automatically chain LLM outputs into the prompt and then into the LLM.\n",
    "\n",
    "You'll see how to define links more explicitly in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"Syntesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "Context:\n",
    "\n",
    "{query_str}\n",
    "\n",
    "Syntesized response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "p = QueryPipeline(\n",
    "    chain=[\n",
    "        prompt_tmpl1, \n",
    "        retriever,\n",
    "        prompt_tmpl2, \n",
    "        Settings.llm, \n",
    "        ], \n",
    "        verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = p.run(topic=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can debug the pipeline by viewing intermediate inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, intermediates  = p.run_with_intermediates(topic=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates['d7612067-3809-4d48-aa49-0c957da8de40']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another RAG Pipeline\n",
    "\n",
    "Here we setup a RAG pipeline without the query rewriting step.\n",
    "\n",
    "Here we need a way to link the input query to both the retriever and summarizer. \n",
    "\n",
    "We can do this by defining a special `InputComponent`, allowing us to link the inputs to multiple downstream modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "input = InputComponent()\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "tree_summarizer = TreeSummarize(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = QueryPipeline(verbose=True, show_progress=True)\n",
    "\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": input,\n",
    "        \"retriever\": retriever,\n",
    "        \"tree_summarizer\": tree_summarizer,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"input\", \"tree_summarizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever\", \"tree_summarizer\", dest_key=\"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = p.run(input=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
