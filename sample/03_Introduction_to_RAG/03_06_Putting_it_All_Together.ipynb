{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 cohere==5.5.0 openai==1.30.1 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-naive\"\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"cohere\", \n",
    "    model=\"command-r-plus\", \n",
    "    api_key=CO_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model_name=\"text-embedding-3-large\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest to VectorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.constants import DEFAULT_CHUNK_SIZE\n",
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "from llama_index.core import StorageContext\n",
    "from utils import ingest\n",
    "\n",
    "print(f\"This is the chunk size: {DEFAULT_CHUNK_SIZE}\")\n",
    "\n",
    "tranforms = [\n",
    "    SentenceSplitter(chunk_size=DEFAULT_CHUNK_SIZE), \n",
    "    Settings.embed_model\n",
    "    ]\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=documents,\n",
    "    transformations=tranforms,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build index over vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_index, create_query_engine\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    "    )\n",
    "\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    embed_model=Settings.embed_model, \n",
    "    vector_store=vector_store, \n",
    "    # storage_context=storage_context\n",
    "    )\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    # llm=Settings.llm\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = query_pipeline.run(input=\"What mental models, principles or philosophies are most valuable for making important life decisions and navigating an uncertain world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1.source_nodes[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = query_pipeline.run(input=\"What is the essence of a life well-lived, and what are the key components of a good life that everyone should strive for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3 = query_pipeline.run(input=\"What is the essence of a life well-lived, and what are the key components of a good life that everyone should strive for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = query_pipeline.run(input=\"What role does luck play in achieving outsized success and wealth creation? Is capitalizing on luck a skill that can be cultivated?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = query_pipeline.run(input=\"What are some commonalities between Naval Ravikant and Nassim Taleb's thinking on luck?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
