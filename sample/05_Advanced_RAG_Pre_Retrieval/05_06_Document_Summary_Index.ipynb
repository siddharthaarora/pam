{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    model=\"gpt-4\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summary Index\n",
    "\n",
    "<img src=\"https://docs.llamaindex.ai/en/stable/_static/production_rag/decouple_chunks.png\" style=\"width:50%; height:5`0%\">\n",
    "\n",
    "Source: [LlamaIndex Documentation](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/#decoupling-chunks-used-for-retrieval-vs-chunks-used-for-synthesis)\n",
    "\n",
    "This method extracts summaries for each document to improve retrieval performance over traditional semantic search on text chunks alone. It uses concise summaries and LLM reasoning capabilities to enhance retrieval before synthesis over retrieved chunks.\n",
    "\n",
    "### üö´ Limitations of chunk-based retrieval\n",
    "\n",
    "- Chunks lack global context \n",
    "\n",
    "- Careful tuning of similarity thresholds required\n",
    "\n",
    "- Embeddings may not capture relevance well\n",
    "\n",
    "- Keyword filtering has its own challenges\n",
    "\n",
    "#### üìù The Document Summary Index stores\n",
    "\n",
    "- A summary extracted by an LLM for each document\n",
    "\n",
    "- The document split into text chunks \n",
    "\n",
    "- Mapping between summaries and source documents/chunks\n",
    "\n",
    "#### üîç Retrieval approaches\n",
    "\n",
    "1. ü§ñ LLM-based: LLM scores relevance of document summaries \n",
    "\n",
    "2. üìê Embedding-based: Retrieve based on summary embedding similarity\n",
    "\n",
    "### ‚öñÔ∏è Advantages\n",
    "\n",
    "- Summaries provide more context than chunks alone\n",
    "\n",
    "- LLM can reason over summaries before full documents\n",
    "\n",
    "- Different optimal representations for retrieval vs. synthesis\n",
    "\n",
    "### üöÄ Key techniques\n",
    "\n",
    "1. Embed summaries linked to document chunks\n",
    "\n",
    "2. Retrieve summaries, replace with full document content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-document-summary-index\"\n",
    "\n",
    "doc_summary_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest using [`DocumentSummaryIndex`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/document_summary/base.py)\n",
    "\n",
    "The `DocumentSummaryIndex`:\n",
    "\n",
    "- üìù Builds an index from a set of documents\n",
    "\n",
    "- üéØ Generates a summary for each document using a response synthesizer\n",
    "\n",
    "- üíæ Stores the summaries and their corresponding document nodes in the index\n",
    "\n",
    "#### üåê Retrieval\n",
    "\n",
    "- Supports two retrieval modes: embedding-based and LLM-based\n",
    "- ü™¢ Embedding-based retrieval:\n",
    "  - Embeds the summaries using an embedding model\n",
    "  - Retrieves relevant summaries based on similarity to a query embedding\n",
    "\n",
    "- üß† LLM-based retrieval:\n",
    "  - Uses a LLM to retrieve relevant summaries based on a query\n",
    "\n",
    "It focuses on indexing documents, generating summaries, and providing efficient retrieval methods based on either embeddings or LLMs. The retriever also supports document management operations like adding and deleting documents from the index.\n",
    "\n",
    "#### The high-level API uses embedding based retrieval by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import DocumentSummaryIndex, get_response_synthesizer\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=16)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    senpai_documents,\n",
    "    llm=Settings.llm,\n",
    "    embed_model=Settings.embed_model,\n",
    "    transformations=[splitter],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    "    vector_store=doc_summary_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setup Query Engine and Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from utils import create_query_engine\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "doc_summaries_query_engine = create_query_engine(\n",
    "    index=doc_summary_index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    )\n",
    "\n",
    "doc_summaries_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We won't run inference using the above as I want to show you the low-level API for embedding based retrieval as well. We'll use that for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú [Document Summary Retrievers](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/document_summary/retrievers.py)\n",
    "\n",
    "\n",
    "<img src=\"https://www.llamaindex.ai/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6d78d199badf9b45f5637d2a87aee0b12b9a335c-2099x1134.png%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75\" style=\"width:70%; height:70%\">\n",
    "\n",
    "Source: [LlamaIndex Blog](https://www.llamaindex.ai/blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec)\n",
    "\n",
    "- üìÇ Contains two types of retrievers:\n",
    "  1. üß† LLM-based retriever (`DocumentSummaryIndexLLMRetriever`)\n",
    "  2. üé® Embedding-based retriever (`DocumentSummaryIndexEmbeddingRetriever`)\n",
    "\n",
    "These document summary retrievers  efficiently retrieve relevant summaries from a document summary index. \n",
    "\n",
    "The LLM-based retriever uses language models to select relevant summaries based on a query, while the embedding-based retriever uses embedding similarity to find relevant summaries. \n",
    "\n",
    "\n",
    "#### üß† [`DocumentSummaryIndexLLMRetriever`](https://github.com/run-llama/llama_index/blob/99984eb87afb2e7feda65d5246ad166b0042f6fe/llama-index-core/llama_index/core/indices/document_summary/retrievers.py#L28)\n",
    "\n",
    "- üìú Retrieves relevant summaries from the index using LLM calls\n",
    "\n",
    "- üéõÔ∏è Customizable prompt for selecting relevant summaries\n",
    "\n",
    "- üç∞ Processes summary nodes in batches\n",
    "\n",
    "- üîù Retrieves top-k summary nodes based on LLM's relevance scoring\n",
    "\n",
    "- ü§ñ Uses an LLM to select relevant summaries\n",
    "\n",
    "##### Arguments you need to know:\n",
    "\n",
    "- `index`:  The index to retrieve from.\n",
    "\n",
    "- `choice_select_prompt`: The prompt to use for selecting relevant summaries. The default prompt can be found [here](https://github.com/run-llama/llama_index/blob/99984eb87afb2e7feda65d5246ad166b0042f6fe/llama-index-core/llama_index/core/prompts/default_prompts.py#L392)\n",
    "\n",
    "- `choice_batch_size`: The number of summary nodes to send to LLM at a time. The default value is 10\n",
    "\n",
    "- `choice_top_k`: The number of summary nodes to retrieve. The default value is 1.\n",
    "\n",
    "- `format_node_batch_fn`: Function to format a batch of nodes for LLM. This defaults to `default_format_node_batch_fn`, which formats a batch of summary nodes by assigning each node a number and joining their contents with a separator.\n",
    "\n",
    "- `parse_choice_select_answer_fn`: Function to parse LLM response. It defaults to `default_parse_choice_select_answer_fn`, which parses the answer string from the LLM, extracting the selected answer numbers and their corresponding relevance scores, and returns them as lists.\n",
    "\n",
    "- `llm` (LLM): The llm to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_llm_retriever = DocumentSummaryIndexLLMRetriever(\n",
    "    doc_summary_index,\n",
    "    choice_top_k=5,\n",
    "    llm=Settings.llm,\n",
    "    # choice_select_prompt=None,\n",
    "    # choice_batch_size=10,\n",
    "    # format_node_batch_fn=None,\n",
    "    # parse_choice_select_answer_fn=None,\n",
    ")\n",
    "\n",
    "doc_llm_query_engine = RetrieverQueryEngine(\n",
    "    retriever=doc_llm_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "doc_llm_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_llm_query_engine.query(\"How can I stop overanalyzing my own moods and feelings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "doc_llm__chain = [input_component, doc_llm_query_engine]\n",
    "\n",
    "doc_llm_query_pipeline = create_query_pipeline(doc_llm__chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_llm_query_pipeline.run(input=\"How can I stop overanalyzing my own moods and feelings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé® [`DocumentSummaryIndexEmbeddingRetriever`](https://github.com/run-llama/llama_index/blob/aad4a6fb94c8fcaf1b7dfac56b88b9e277886bfe/llama-index-core/llama_index/core/indices/document_summary/retrievers.py#L121)\n",
    "\n",
    "- üìú Retrieves relevant summaries from the index using embedding similarity\n",
    "\n",
    "- üî¢ Retrieves top-k summary nodes based on embedding similarity\n",
    "\n",
    "- ü™¢ Uses an embedding model to embed the query\n",
    "\n",
    "- üìè Queries the vector store to find similar summaries\n",
    "\n",
    "##### Arguments you need to know\n",
    "\n",
    "- `index`: The index to retrieve from.\n",
    "\n",
    "- `similarity_top_k`: The number of summary nodes to retrieve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexEmbeddingRetriever\n",
    "\n",
    "doc_embed_retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    doc_summary_index,\n",
    "    # similarity_top_k=1,\n",
    ")\n",
    "\n",
    "doc_embed_query_engine = RetrieverQueryEngine(\n",
    "    retriever=doc_embed_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embed__chain = [input_component, doc_embed_query_engine]\n",
    "\n",
    "doc_embed_query_pipeline = create_query_pipeline(doc_embed__chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embed_query_pipeline.run(input=\"How can I stop overanalyzing my own moods and feelings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
