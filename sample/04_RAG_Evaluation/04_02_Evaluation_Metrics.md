# üìê Evaluation Metrics

A robust RAG evaluation requires carefully measuring the retriever and generator performance.

It is essential to use the right metrics and construct datasets that match how the system will be used in the real world. RAG evaluation is an area of active research. There's plenty of new metrics, frameworks, and libraries popping up for RAG evalution. I won't cover them all in this course, and instead focus on the following core metrics.

**üéØ Core Metrics for RAG Evaluation:**

1. **ü§ù Faithfulness:**

   - Ensures answers generated by the model remain true to the given context

   - Answers must be consistent with the context information and not deviate or contradict it

   - Crucial for addressing illusions in large models

2. **üéØ Answer Relevance:**

   - Generated answers need to be directly related to the posed question

   - Ensures the model stays on topic and provides pertinent information

3. **üìë Context Relevance:**

   - Retrieved contextual information should be accurate and targeted

   - Avoids irrelevant content that can reduce the efficiency of LLMs in utilizing context

   - Processing long texts is costly for LLMs, so minimizing irrelevant information is essential

4. **üìä Mean Reciprocal Rank (MRR):**

  - Statistic measure for evaluating processes that produce a list of possible responses to a sample of queries, ordered by probability of correctness

  - Reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer

  - MRR is the average of the reciprocal ranks of results for a sample of queries

# üë®üèΩ‚Äç‚öñÔ∏è LLM-as-a-Judge 

Faithfulness, context relevancy, and answer relevancy use the [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) paradigm for evaluation.

LLM-as-a-Judge is a technique that uses an LLM to automatically evaluate the outputs of other LLMs in a way that aims to approximate human judgment. The key idea is to leverage strong LLMs to assess generated text that usually requires human evaluation, such as the abovementioned metrics. Using LLMs as judges is a scalable and cost-effective way to evaluate RAG systems. It enables faster benchmarking and iteration cycles during AI development.

However, LLM judges have some limitations and biases that need to be carefully addressed, such as:

- Position bias: Favoring certain positions in a text

- Verbosity bias: Preferring longer outputs

- Limited reasoning capabilities for complex topics like math

Despite these challenges, studies have shown that with proper prompt engineering and debiasing techniques, strong LLM judges like GPT-4 can achieve high agreement (>80%) with human preferences and expert ratings across different benchmarks. LLM-as-a-Judge is a promising complement to human evaluation for assessing open-ended AI tasks at scale.


## But first, we need an evaluation dataset.

Which we'll create in the next lesson.

## More resources for RAG Evaluation

- [ARAGOG: Advanced RAG Output Grading](https://arxiv.org/abs/2404.01037)

- [ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2311.09476)

- [Evaluation of Retrieval-Augmented Generation: A Survey](https://arxiv.org/abs/2405.07437) and the accompanying [GitHub repository](https://github.com/YHPeter/Awesome-RAG-Evaluation)

- [RAG Evaluation, Hugging Face Blog](https://huggingface.co/learn/cookbook/en/rag_evaluation)

- [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)

- [Using LLM-as-a-judge üßë‚Äç‚öñÔ∏è for an automated and versatile evaluation, Hugging Face Blog](https://huggingface.co/learn/cookbook/en/llm_judge)

-[Streamlining RAG evaluation, Derek Haynes](https://dlite.cc/2023/10/04/2023-eval-rag-apps.html)