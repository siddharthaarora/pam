{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(provider=\"openai\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"node-postprocessors\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Postprocessors \n",
    "\n",
    "A postprocessor is a tool that applies some additional processing or filtering to a list of nodes returned by a query and returns the final results. Node postprocessors are modules that take a set of nodes, apply some kind of transformation or filtering, and return them.\n",
    "\n",
    "In LlamaIndex, node postprocessors are commonly used within a query engine after the node retrieval step and before the response synthesis step.\n",
    "\n",
    "Here, we'll discuss the following postprocessors (you can find the source code for all of these [here](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/postprocessor/node.py#L64C1-L140C17))\n",
    "\n",
    " - üë• `SimilarityPostprocessor`\n",
    "\n",
    " - üîë `KeywordNodePostprocessor`\n",
    "\n",
    " - üîÄ `MetadataReplacementPostProcessor`\n",
    "\n",
    " - ‚Ü∏ `LongContextReorder`\n",
    " \n",
    " - üìù `SentenceEmbeddingOptimizer`\n",
    "\n",
    "Node Postprocessors discussed here are instances of the [`BaseNodePostProcessor`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/postprocessor/types.py) base class that provides a structure for creating custom post-processors in a query pipeline. \n",
    "\n",
    "This means they all have a common API and usage pattern.\n",
    "\n",
    "#### ‚†∑ Usage Pattern\n",
    "\n",
    "Node postprocessors are used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n",
    "\n",
    "A really simple usage pattern is like so:\n",
    "\n",
    "```python\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.postprocessor import YourPostProcessOfChoice\n",
    "\n",
    "... # prior steps to this point: index is defined\n",
    "\n",
    "your_post_processor = YourPostProcessOfChoice(WhateverArgumentsYouNeedToPass)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    ..., #all your othe query engine arguments\n",
    "    node_postprocessors = your_post_processor\n",
    ")\n",
    "\n",
    "# for just a single query\n",
    "response = query_engine.query(\"your query\")\n",
    "\n",
    "# use the query pipeline and apply to your entire dataset if you'd like\n",
    "```\n",
    "\n",
    "##### üëâüèº  By this point, after seeing the pattern a dozen or more times, I hope you've picked up on it and can apply it yourself. I'll describe the post processors and how to instantiate them, but you can apply them to your query engine and use them on your own. I believe in you!\n",
    "\n",
    "## üë• [`SimilarityPostprocessor`](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-core/llama_index/core/postprocessor/node.py#L64C1-L140C17)\n",
    "\n",
    "\n",
    "This does what it says on the tin. \n",
    "\n",
    "\n",
    "It filters a list of nodes based on their similarity scores. You provide a `similarity_cutoff` value to control the filtering threshold. Nodes with similarity scores above the cutoff (or all nodes if no cutoff is provided) are included in the output list.\n",
    "\n",
    "That's all there is to it.\n",
    "\n",
    "##### Arguments you need to know\n",
    "\n",
    "- `similarity_cutoff`: The minimum similarity score required for a node to be included in the output. If not provided, no filtering is performed. Which is kinda pointless to do, so if you use this then make sure you provide a value.\n",
    "\n",
    "##### Under the hood\n",
    "\n",
    "Not satisfied with that explanation from above? Ok, let's go more in depth...\n",
    "\n",
    "1. The method checks if `similarity_cutoff` is set (not `None`).\n",
    "\n",
    "2. It iterates over each node in the input list.\n",
    "\n",
    "3. For each node, it checks if the node's similarity score is above the `similarity_cutoff` (if set).\n",
    "\n",
    "   - If the similarity score is `None` or below the cutoff, the node is not included in the output list.\n",
    "\n",
    "   - If the similarity score is above the cutoff or if `similarity_cutoff` is not set, the node is included in the output list.\n",
    "\n",
    "4. The method returns the filtered list of `NodeWithScore` objects that passed the similarity cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "similarity_postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîë[`KeywordNodePostprocessor`](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-core/llama_index/core/postprocessor/node.py#L20)\n",
    "\n",
    "This filters nodes based on the presence or absence of specific keywords. You provide a list of required keywords and/or a list of exclude keywords. Nodes that contain all the required keywords and none of the exclude keywords are included in the output list.\n",
    "\n",
    "#### Arguments you need to know\n",
    "\n",
    "- `required_keywords`: A list of keywords that must be present in a node's content for it to be included in the output. If not provided, this criterion is not applied.\n",
    "\n",
    "- `exclude_keywords`: A list of keywords that must not be present in a node's content for it to be included in the output. If not provided, this criterion is not applied.\n",
    "\n",
    "- `lang`: The language of the text in the nodes. Default is \"en\" (English).\n",
    "\n",
    "#### Under the hood\n",
    "\n",
    "1. The method iterates over each `NodeWithScore` object in the input list.\n",
    "\n",
    "2. For each node, it retrieves the node's content and processes it using the Spacy language model.\n",
    "\n",
    "3. If `required_keywords` are provided and the node's content does not match any of the required keywords, the node is skipped.\n",
    "\n",
    "4. If `exclude_keywords` are provided and the node's content matches any of the exclude keywords, the node is skipped.\n",
    "\n",
    "5. If a node passes both the required and exclude keyword checks (or if no keywords are provided), it is included in the output list.\n",
    "\n",
    "6. The method returns the filtered list of `NodeWithScore` objects that passed the keyword criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
    "\n",
    "required_keywords = [\"luck\", \"destiny\", \"hardwork\", \"mind\", \"decision\"]\n",
    "\n",
    "exclude_keywords = [\"Trulia\", \"Redfin\", \"Zillow\"]\n",
    "\n",
    "keyword_postprocessor = KeywordNodePostprocessor(\n",
    "    required_keywords=required_keywords, exclude_keywords=exclude_keywords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_engine\n",
    "from utils import create_query_pipeline\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    post_processor=keyword_postprocessor,\n",
    "    return_source=True\n",
    "    )\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "query_chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(query_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How to make good decisions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline.run(input=\"How to make good decisions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÄ [`MetadataReplacementPostProcessor`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/postprocessor/metadata_replacement.py)\n",
    "\n",
    "It allows you to replace the content of each node in the input list with a specific metadata key value, instead of the original content.\n",
    "\n",
    "If the specified field is not present in the metadata, then the original node content remains unchanged. This feature is most useful when combined with the `SentenceWindowNodeParser`. \n",
    "\n",
    "#### Arguments you need to know\n",
    "\n",
    "- `target_metadata_key`: The metadata key whose value will replace the node's content.\n",
    "\n",
    "Under the hood\n",
    "\n",
    "1. It iterates over each `NodeWithScore` object in the input list.\n",
    "2\n",
    ". For each node, it retrieves the value of the specified `target_metadata_key` from the node's metadata.\n",
    "   - If the metadata key exists, its value is used to replace the node's content.\n",
    "   - If the metadata key does not exist, the node's content remains unchanged.\n",
    "\n",
    "3. The method returns the modified list of `NodeWithScore` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postprocessor = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚Ü∏ [`LongContextReorder`](https://github.com/run-llama/llama_index/blob/236fae0e8a30d6f8f9ac31777868d1d868933d79/llama-index-core/llama_index/core/postprocessor/node.py#L358)\n",
    "\n",
    "[According to a study](https://arxiv.org/abs/2307.03172), language models often struggle to access significant details that are located in the middle of an extended context.\n",
    "\n",
    " The study found that the best performance is achieved when important information is positioned at the beginning or end of the input context. Even models designed for long contexts experience a notable drop in performance as the input context lengthens. \n",
    "\n",
    "To address this issue, the `LongContextReorder` class reorders the nodes based on their relevance scores, which is helpful in cases where a large top-k is needed. It alternately places the nodes with the highest scores at the beginning and end of the list, with the goal of improving the model's ability to access important information in long contexts. \n",
    "\n",
    "This is done to address the issue highlighted in the mentioned study, where models struggle to access important information in the middle of long contexts.\n",
    "\n",
    "#### Arguments you need to know\n",
    "\n",
    "You don't need to pass any arguments.\n",
    "\n",
    "#### Under the hood\n",
    "\n",
    "1. It sorts the input `nodes` list based on their relevance scores in descending order (highest score first). If a node's score is `None`, it is treated as 0.\n",
    "\n",
    "2. It iterates over the sorted list.\n",
    "\n",
    "3. For each node, it checks if the index is even (i.e., 0, 2, 4, ...) or odd (i.e., 1, 3, 5, ...).\n",
    "\n",
    "   - If the index is even, the node is inserted at the beginning of the `reordered_nodes` list using the `insert` method with index 0. This ensures that nodes with even indices are placed at the beginning of the list.\n",
    "\n",
    "   - If the index is odd, the node is appended to the end of the `reordered_nodes` list using the `append` method.\n",
    "\n",
    "4. Finally, the method returns the `reordered_nodes` list containing the nodes in the new order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LongContextReorder\n",
    "\n",
    "postprocessor = LongContextReorder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù [`SentenceEmbeddingOptimizer`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/postprocessor/optimizer.py)\n",
    "\n",
    "The `SentenceEmbeddingOptimizer` is a node postprocessor that improves the text content of nodes based on their relevance to a given query. \n",
    "\n",
    "It uses embedding-based similarity scores to select the most relevant sentences and shorten the input text. The optimizer removes sentences that are not related to the query using embeddings. The percentile cutoff determines the top percentage of relevant sentences to be used. \n",
    "\n",
    "Alternatively, the threshold cutoff can be specified to select which sentences to keep based on a raw similarity cutoff.\n",
    "\n",
    "# Arguments you need to know\n",
    "\n",
    "All of these arguments are optional and you can combine them as you see fit.\n",
    "\n",
    "- `embed_model`: The embedding model to use for generating sentence embeddings. Defaults to `OpenAIEmbedding` if not provided.\n",
    "\n",
    "- `percentile_cutoff`: The percentile cutoff for selecting the top sentences based on their similarity scores. The number of sentences selected is calculated as `int(len(sentences) * percentile_cutoff)`.  Ex, if `percentile_cutoff` is set to 0.5, it means that the top 50% of sentences with the highest similarity scores will be selected. If you want to select a fixed percentage of the most relevant sentences, use `percentile_cutoff`.\n",
    "\n",
    "- `threshold_cutoff`: The threshold cutoff for selecting sentences based on their similarity scores. Ex, if `threshold_cutoff` is set to 0.7, only sentences with a similarity score greater than or equal to 0.7 will be selected. If you want to select sentences that meet a minimum similarity score criterion, use `threshold_cutoff`. You can, of course, combine this together with `percentile_cutoff`.\n",
    "\n",
    "- `tokenizer_fn`: A function to split the text into sentences. Defaults to the NLTK English tokenizer if not provided.\n",
    "\n",
    "- `context_before`: The number of sentences to include before the selected relevant sentences for additional context.\n",
    "\n",
    "- `context_after`: The number of sentences to include after the selected relevant sentences for additional context.\n",
    "\n",
    "#### Under the hood\n",
    "For each node:\n",
    "\n",
    "1. Retrieves the text content of the node.\n",
    "\n",
    "2. Splits the text into sentences using the tokenizer function.\n",
    "\n",
    "3. Generates embeddings for the query (if not provided) and the sentences.\n",
    "\n",
    "4. Calculates the similarity scores between the query embedding and sentence embeddings.\n",
    "\n",
    "5. Selects the top sentences based on the percentile cutoff and/or threshold cutoff.\n",
    "\n",
    "6. Retrieves the context sentences before and after the selected sentences.\n",
    "\n",
    "7. Joins the selected sentences and their context to form the optimized text.\n",
    "\n",
    "8. Sets the optimized text as the new content of the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceEmbeddingOptimizer\n",
    "from utils import setup_embed_model\n",
    "\n",
    "postprocessor = SentenceEmbeddingOptimizer(\n",
    "    embed_model=Settings.embed_model,\n",
    "    percentile_cutoff=0.5,\n",
    "    threshold_cutoff=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You cannot pass a `node_postprocessor` directly to as_retriever.\n",
    "\n",
    "In the LlamaIndex framework, node postprocessors are configured and used within the context of a `QueryEngine` rather than directly with the retriever. The retriever is responsible for fetching the most relevant nodes based on the query, and the node postprocessors are then applied within the `QueryEngine` to further refine, filter, or augment these nodes before the final response synthesis.\n",
    "\n",
    "Here's how you typically set up a QueryEngine with node postprocessors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Assuming 'index' is already created and configured\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10\n",
    ")\n",
    "\n",
    "# Configure node postprocessors\n",
    "node_postprocessors = [\n",
    "    SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "]\n",
    "\n",
    "# Assemble the Query Engine with the retriever and node postprocessors\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "# Now you can use this query engine to process queries\n",
    "response = query_engine.query(\"Your query here\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
